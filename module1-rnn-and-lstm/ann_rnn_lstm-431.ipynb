{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skhabiri/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/LS_DS17_431_RNN_and_LSTM_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7QXzrvrSjru"
   },
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_5DZJg0Sjrw"
   },
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - **understand how data varies over time (or any sequential order), and use the order/time dimension predictively.**\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "**A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\"** - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)\n",
    "**RNN stands for Recurrent Neural Network. An LSTM is a Long Short-Term Memory layer, which is a type of RNN.\n",
    "These layers, built into a network, function as some sort of memory that allows the network to infer from not only the present, but also past events. They do this by keeping a sort of “internal memory” that they modify to keep track of meaningful events that happened.\n",
    "So really, an LSTM network is a quite common type of RNN. It has been widely adopted since the original RNN layer would tend overwrite too much of its “internal memory” at each step, losing the ability to infer from events further in past. The LSTM layer architecture is instead built in such a way the the network “decides” whether to modify its “internal memory” at each step. Doing so, and if properly trained, the layer can keep track of important events from further in the past, allowing for much richer inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fkzy3-FDSjr1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. **The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.**\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved. $C_{t-1}$ can be chosen to pass through to $C_{t+1}$\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - **language is inherently ordered data (letters/words go one after another, and the order *matters*).** [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h-vcbT3r4M3"
   },
   "source": [
    "* In the above diagram the path from input sequence of Xt-1, X, Xt+1 to purple lstm cells and to pink output neurons are all parallel (the vertical path in diagram) while lstm cells they do interact with each other in series manner (the horizental path). The horizental path could be bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RgdZYqBadli6",
    "outputId": "6b7aa5a6-c01d-4bae-a604-3b5cfdcd16b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vanishing gradients similar to vanishing the history of sequence in future predictions\n",
    "0.0001 ** 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKrg7cMKSjr8"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9IYScGvBT1g"
   },
   "source": [
    "## Steps for text setiment classification:\n",
    "* Read 25K imbd reviews, with a bag of 20K words as our dictionary of words >> `x_train`\n",
    "* each word is represented by its relative count in the entire dataset as its integer representation\n",
    "* use sequence.pad_sequences to fix the size of each review to 80 words\n",
    "* In building the model use tf.keras.layers.Embedding layer to learn the spacial location of a dense vector of size `128` for each word in 20K dictionary in the context of all the reviews.\n",
    "* Use LSTM layer to build up the model and train it based on the binary sentiment in y label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "e72c00f4-5218-4a61-e877-2f44c3535eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000,) train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "\"\"\" max_features is number of the most frequent words \n",
    "collected in the bag of words. The associated number to \n",
    "each word refers to the count of that word in the dataset.\n",
    "\"\"\"\n",
    "max_features = 20000\n",
    "\n",
    "# desired number of words per review\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 25000 reviews\n",
    "print(x_train.shape, 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0]\n",
      "218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670,     2,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167,     2,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4,     2,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(y_train[:10])\n",
    "print(len(x_train[0]))\n",
    "np.asarray(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19999)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word ids are assigned in a sequence from 1 to 19999\n",
    "min([min(i) for i in x_train]), max([max(i) for i in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "JHSOnf3ASjsJ",
    "outputId": "212f6e2e-b8e6-4a5c-b3f2-82a1d9c9498d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     4, ..., 19997, 19998, 19999])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each review is a list of word ids from 20K bag of words. \n",
    "unique_words = np.unique(np.concatenate(x_train))\n",
    "print(len(unique_words))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QpMv7Ha8iSDZ",
    "outputId": "f435369b-c5ad-4121-f4cf-32710df0419b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (25000,)\n",
      "x_test shape:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Prep-padding shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Xmm5Q6BkiW-o",
    "outputId": "aa89ae21-71d3-4048-9f6c-e8566a00fe21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in first review:  218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4       15\n",
       "16      11\n",
       "5        9\n",
       "12       6\n",
       "22       6\n",
       "        ..\n",
       "92       1\n",
       "224      1\n",
       "100      1\n",
       "3941     1\n",
       "98       1\n",
       "Length: 123, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# repeated words in each review\n",
    "r0 = pd.Series(x_train[0]).value_counts()\n",
    "print(\"# of words in first review: \",r0.sum())\n",
    "r0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K_Uu_xSUibaY",
    "outputId": "eea7bc9c-4e66-40da-9733-1f74bd61487c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in x_train[:10]]  # num of words in the first ten reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "zFHhoTSAio7u",
    "outputId": "8d4a839b-1981-415e-880d-a04386bfeb24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 778,\n",
       " 128,\n",
       " 74,\n",
       " 12,\n",
       " 630,\n",
       " 163,\n",
       " 15,\n",
       " 4,\n",
       " 1766,\n",
       " 7982,\n",
       " 1051,\n",
       " 2,\n",
       " 32,\n",
       " 85,\n",
       " 156,\n",
       " 45,\n",
       " 40,\n",
       " 148,\n",
       " 139,\n",
       " 121,\n",
       " 664,\n",
       " 665,\n",
       " 10,\n",
       " 10,\n",
       " 1361,\n",
       " 173,\n",
       " 4,\n",
       " 749,\n",
       " 2,\n",
       " 16,\n",
       " 3804,\n",
       " 8,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 12,\n",
       " 43,\n",
       " 127,\n",
       " 24,\n",
       " 15344,\n",
       " 10,\n",
       " 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6th review is short\n",
    "x_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.preprocessing.sequence()`:\n",
    "> This function transforms a list (of length `num_samples`) of sequences into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
    "`num_timesteps` is either the `maxlen` argument if provided, or the length of the longest sequence in the list.\n",
    "* Signature:\n",
    "```\n",
    "sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ic0jzRvzSjsQ",
    "outputId": "ba12b679-86cb-4ce5-f950-f63fd81550fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "#.pad_sequence method truncates or pads from the beginning\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "rwUsR-PwwS1Q",
    "outputId": "671c1a75-b804-4277-8901-76ed73b3abbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,   778,   128,    74,    12,   630,   163,    15,\n",
       "           4,  1766,  7982,  1051,     2,    32,    85,   156,    45,\n",
       "          40,   148,   139,   121,   664,   665,    10,    10,  1361,\n",
       "         173,     4,   749,     2,    16,  3804,     8,     4,   226,\n",
       "          65,    12,    43,   127,    24, 15344,    10,    10],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMvE12pjx-4v"
   },
   "source": [
    "### 1. Word Embedding\n",
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "\n",
    "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, **in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "The position of a word in the learned vector space is referred to as its embedding.\n",
    "Two popular examples of methods of learning word embeddings from text include: Word2Vec. GloVe.\n",
    "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.**\n",
    "\n",
    "### 2. Keras Embedding Layer\n",
    "**Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.**\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "* `input_dim`: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* `output_dim`: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* `input_length`: This is the dimension of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 80)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "MHDK11zsSjsh",
    "outputId": "d546cdcb-5c03-4978-a86b-f432e46f858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# we are predicting the imdb review sentiment (output size =1)\n",
    "\n",
    "\"\"\"\n",
    "Embedding Layer:\n",
    "input_dim: size of dictionary\n",
    "output_dim: size of embedding vector for each word in teh sequence\n",
    "input_length: number of words in one sequence, # of time steps\n",
    "\n",
    "a sequence is a combination of 20K words.\n",
    "we need 20k*128=256k parameters to embed each sequence\n",
    "\"\"\"\n",
    "\n",
    "model.add(Embedding(input_dim= max_features, output_dim = 128, input_length=maxlen))\n",
    "\n",
    "# alternatively: dynamically takes the input shape of 80\n",
    "# model.add(Embedding(max_features, 128))\n",
    "\n",
    "# LSTM input: 128 + 1(bias)\n",
    "# LSTM output: 128 (arbitrary)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# output is a binary classification representing the sentiment of the review\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# in Output Shape column (None, 80, 128) is (batch_size, #of words to embed, embedding vector size(128))\n",
    "# The most left None in Shape column is the dynamic batch_size for all the layers\n",
    "# the shape with None is dynamic and can be inferred\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131584"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM params = 4 * ((size_of_input + 1) * size_of_output + size_of_output^2)\n",
    "4*((128+1)*128+128**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "fgC8gsgYSjso",
    "outputId": "2b019884-bd09-4d66-a7ef-3d26d90122b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 98s 122ms/step - loss: 0.4975 - accuracy: 0.7398 - val_loss: 0.3657 - val_accuracy: 0.8375\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 0.2456 - accuracy: 0.9037 - val_loss: 0.4008 - val_accuracy: 0.8302\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 94s 120ms/step - loss: 0.1506 - accuracy: 0.9450 - val_loss: 0.5058 - val_accuracy: 0.8260\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 92s 118ms/step - loss: 0.0957 - accuracy: 0.9662 - val_loss: 0.5636 - val_accuracy: 0.8165\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.5928 - val_accuracy: 0.8165\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this ANN do?\n",
    "* Embedding layer: It takes an input of word sequence with length of 80 (# of time steps) and Bag of Word of 20K and converts it to a 3 dimension tensor of (batch_size, time_steps(80), features(20k)). It creates a blueprint network layer to output a tensor with 3 dimension (batch_size, time_stesp(80), embedding_dim(128)).\n",
    "\n",
    "* LSTM layer: The input tensor to the LSTM is (batch_size, time_steps(80), embedding_size(128)). LSTM evaluates the relationship between 80 words in the sequence with short term and longterm memory technique and allow back propagation to correct the weights throughout the entire network to contextualize the embeddings accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4237242043018341,\n",
       "  0.252041757106781,\n",
       "  0.16468730568885803,\n",
       "  0.10760781913995743,\n",
       "  0.07354194670915604],\n",
       " 'accuracy': [0.8028799891471863,\n",
       "  0.8990399837493896,\n",
       "  0.9374799728393555,\n",
       "  0.9622399806976318,\n",
       "  0.973800003528595],\n",
       " 'val_loss': [0.36566397547721863,\n",
       "  0.4007636606693268,\n",
       "  0.5058155655860901,\n",
       "  0.563585638999939,\n",
       "  0.5927637219429016],\n",
       " 'val_accuracy': [0.8374800086021423,\n",
       "  0.8301600217819214,\n",
       "  0.8259999752044678,\n",
       "  0.8164799809455872,\n",
       "  0.8164799809455872]}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicorns.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "yfydL-ZRSjst",
    "outputId": "e13ec026-896e-46be-c0ce-09692bbbcd4e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **model.predict require an input of (batch_size=None, 80). Passing x_test[0] directly to .predict method will be considered (batch_size=80, 1). Hence we either pass input slice x_test[:1] or expand_dims with axis=0 to call the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (1, 80))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].shape, x_test[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     1,   591,   202,    14,    31,     6,\n",
       "         717,    10,    10, 18142, 10698,     5,     4,   360,     7,\n",
       "           4,   177,  5760,   394,   354,     4,   123,     9,  1035,\n",
       "        1035,  1035,    10,    10,    13,    92,   124,    89,   488,\n",
       "        7944,   100,    28,  1668,    14,    31,    23,    27,  7479,\n",
       "          29,   220,   468,     8,   124,    14,   286,   170,     8,\n",
       "         157,    46,     5,    27,   239,    16,   179, 15387,    38,\n",
       "          32,    25,  7944,   451,   202,    14,     6,   717],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing the batch_size dimension: 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13265833],\n",
       "       [0.65723205],\n",
       "       [0.40162528],\n",
       "       [0.5781395 ],\n",
       "       [0.6550277 ],\n",
       "       [0.57304525],\n",
       "       [0.325602  ],\n",
       "       [0.46977627],\n",
       "       [0.51240087],\n",
       "       [0.42681643]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrong predictions as the input is not correct format\n",
    "# This is a wrong way and takes 80 input features as batch_size=80\n",
    "print(\"missing the batch_size dimension:\",len(model.predict(x_test[0]))) \n",
    "model.predict(x_test[0])[-10:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QW4rh_-sGsos",
    "outputId": "e1ee5baa-c6aa-47f8-c3a8-9595b8af6c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24344578]]\n",
      "[[0.24344578]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.24344578]], dtype=float32)>,\n",
       " 0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query = np.expand_dims(a=x_test[0],axis=0)\n",
    "print(model.predict(query))\n",
    "print(model.predict(x_test[:1]))  \n",
    "\n",
    "# while the .predict(query) method returns a np.ndarray, model(query) returns a tensor\n",
    "model(x_test[:1]), y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVFa0NC7Sjs1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3RkqrGT1jRl"
   },
   "source": [
    "## Steps to generate Text:\n",
    "* Import articles. (`137` articles in 137 rows)\n",
    "* create one giant text of all the articles >> `text`\n",
    "* create a bag of unique characters with a corresponding unique integer >> `char, char_int, int_char`\n",
    "* create a list of interleaved sequences of the same length characters, `40` represented with integer keys of the characters as input data and a list of corresponding integer represented next_character (the charcater after the sequence) as the target label >> `sequences, next_char`\n",
    "> we can't train on this data as the next character *prediction* is a floating point that if it's rounded up or down, does not necessarily point to a predicted character as the integer representation of characters are random not ordinal\n",
    "* create multi dimensional boolean array for X: \n",
    "1. axis0: sequence number >> size `178374`\n",
    "2. axis1: position of the character in sequence >> size `40`\n",
    "3. axis2: identifier of the character in the bag of characters >> size `121`\n",
    "* create a multi dimensional boolean array for Y:\n",
    "1. axis0: sequence number >> size `178374`\n",
    "2. axis1: identifier of the next_char in the bag of characters >> size `121`\n",
    "* Build the LSTM model\n",
    "* define a callback at the end of each epoch:\n",
    "1. pick a random prompt (index) in the concatinated giant `text`\n",
    "2. grab the 40 characters of the `text` as the query seed for character generation\n",
    "3. convert the query seed to X.shape, i.e. (1, 40, 121) >> `x_pred`\n",
    "4. get a y prediction from model after each epoch training, shape: (1, 121) >> `preds` an array of 121 floating values beytween 0 and 1, 1 being the strongest possibility for being the next_char\n",
    "5. scale the values of y array to proba and take one *draw* from the array considering the value of proba. grab the selected char from draw and prints it as the next char after the sequence.\n",
    "6. shift the input sequence to the right by 1 and predict the next char again >> get `400` characters iteratively\n",
    "* This way we work with probability of all the characters as the next char instead of a floating number that is supposed to resemble one of the characters integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-kuJzHnSjs3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOWH0qNC6QuV"
   },
   "source": [
    "* The dataset consists of 136 new articles each article in one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "CB2EzxcmmWhE",
    "outputId": "46774a35-45d5-46f0-8a39-81590c56cb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When President Trump announced his decision to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Like an aging rock star, the president is now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article\n",
       "0    Contributing columnist\\n\\nThe House is on fire...\n",
       "1    When President Trump announced his decision to...\n",
       "10   Russian President Vladimir Putin speaks at a s...\n",
       "100  “The Queen’s Speech” is designed to acknowledg...\n",
       "101  Like an aging rock star, the president is now ..."
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "    'https://raw.githubusercontent.com/skhabiri/ML-DeepLearning/main/module1-rnn-and-lstm/wp_articles.json')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10-d1GUVSjtC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29.txt', '15.txt', '114.txt', '100.txt', '128.txt', '129.txt', '101.txt', '115.txt', '14.txt', '28.txt', '16.txt', '103.txt', '117.txt', '116.txt', '102.txt', '17.txt', '13.txt', '106.txt', '112.txt', '113.txt', '107.txt', '12.txt', '10.txt', '38.txt', '111.txt', '105.txt', '104.txt', '110.txt', '39.txt', '11.txt', '76.txt', '62.txt', '89.txt', '88.txt', '63.txt', '77.txt', '49.txt', '61.txt', '75.txt', '74.txt', '60.txt', '48.txt', '64.txt', '70.txt', '58.txt', '59.txt', '71.txt', '65.txt', '73.txt', '67.txt', '9.txt', '98.txt', '99.txt', '8.txt', '66.txt', '72.txt', '57.txt', '5.txt', '43.txt', '94.txt', '80.txt', '81.txt', '95.txt', '42.txt', '56.txt', '4.txt', '68.txt', '40.txt', '6.txt', '54.txt', '83.txt', '97.txt', '96.txt', '82.txt', '7.txt', '55.txt', '41.txt', '69.txt', '45.txt', '51.txt', '3.txt', '79.txt', '86.txt', '92.txt', '93.txt', '87.txt', '78.txt', '50.txt', '2.txt', '44.txt', '0.txt', '52.txt', '46.txt', '91.txt', '85.txt', '84.txt', '90.txt', '47.txt', '1.txt', '53.txt', '34.txt', '20.txt', '135.txt', '121.txt', '109.txt', '108.txt', '120.txt', '134.txt', '21.txt', '35.txt', '23.txt', '37.txt', '122.txt', '123.txt', '36.txt', '22.txt', '26.txt', '32.txt', '127.txt', '133.txt', '132.txt', '126.txt', '33.txt', '27.txt', '31.txt', '25.txt', '19.txt', '118.txt', '130.txt', '124.txt', '125.txt', '131.txt', '119.txt', '18.txt', '24.txt', '30.txt']\n",
      "\n",
      "number of articles: 136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, we'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you're running locally:\n",
    "\n",
    "data_files = os.listdir('./articles')  \n",
    "print(data_files)\n",
    "\n",
    "# Read in Data\n",
    "data1 = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data1.append(f.read())\n",
    "\n",
    "print(\"\\nnumber of articles:\", len(data))\n",
    "data1[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "H-Rw-vNT5ImV",
    "outputId": "32e0a516-dcc6-49c2-fd80-efcbce3c1e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin speaks at a summit in Turkmenistan on Friday. (Alexei Druzhinin/Sputnik/Kremlin Pool/AP)\\n\\nWith Mariana Alfaro\\n\\nTHE BIG IDEA: Vladimir Putin has won so much these past three years that he may get tired of winning.\\n\\nThe U.S. intelligence community’s January 2017 report on Russian interference in the previous year’s presidential campaign sought to explain why Donald Trump was so attractive to Moscow. This sentence has fresh salience: “Pro-Kremlin proxy Vladimir Zhir'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 500 characters of the third article\n",
    "df['article'][df['article'].index[2]][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lL56kYH_SjtK",
    "outputId": "0d999f7d-b947-4cc8-9c3d-108210f2fba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas series into a ndarray of 136 string elements\n",
    "data = df['article'].values\n",
    "print(data.shape, type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "zYu5BO-_SjtQ",
    "outputId": "8150e5c4-57c5-4ce2-fcb2-945a85ad42d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin speaks at a summit in Turkmenistan on Friday. (Alexei Druzhinin/Sputnik/Kremlin Pool/AP)\\n\\nWith Mariana Alfaro\\n\\nTHE BIG IDEA: Vladimir Putin has won so much these past three years that he may get tired of winning.\\n\\nThe U.S. intelligence community’s January 2017 report on Russian interference in the previous year’s presidential campaign sought to explain why Donald Trump was so attractive to Moscow. This sentence has fresh salience: “Pro-Kremlin proxy Vladimir Zhir'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data's implicit indices are reset\n",
    "data[2][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_MBOMMIOLAd"
   },
   "source": [
    "### Bag of characters in the form of a dictionary with corresponding integers assigned\n",
    "* Flatten the dataset into a giant string named text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75QPh0KzSjtU"
   },
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "#.join is a string method. joining each row of the ndarray to another row with a white space\n",
    "# join iterables of data with a space\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters, enumerate(set(text)) would produce random indexing at every run\n",
    "# therefore make a list of the set\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables \n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4wmmFY5ODT0K",
    "outputId": "28b7a9fe-02e9-4a75-f1b9-40a9b57b0809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ 8 z ’ V k * t 🗣 C $ é U O \\xad 3 + a m & ! è B - [ ⅔ K ã w R \\u2066 i M ‘ ó — g Z N W p F : \\u2069 @ L S { , · 0 \\' b © H ” ) q A / x y G 6 – 1 E ⭐ ê X 2 l J . í “ ﬂ ― 4 u j P ● × v T h D \\n 5 e ] r 7 \" I f ;   ? Y 🤔 ñ 👻 o • á ½ … 9 d s ( # | % ⅓ n c ö Q'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of characters\n",
    "\" \".join(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cO8fk5KU-hLc",
    "outputId": "24f81eb5-1304-4480-8ff9-d7b3e1769194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891910 <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Contributing columnist\\n\\nThe House is on fire. And '"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text), type(text))\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZZ1UHyJuEO55",
    "outputId": "1067383c-ed61-478c-d5fd-b4f7fa360ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "# number of characters\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qJW9phVonGhK",
    "outputId": "1274ff3e-5880-4909-a698-edb55bac1f59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_int[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ginZieymnNdY",
    "outputId": "d11d7319-c94c-485b-9bde-a5574aec7f44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_char[57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO5qC5K-P-bG"
   },
   "source": [
    "### Create same length sequences of the data\n",
    "* sample the character sequence of 40 (length of sequence) consecutive characters every 5 character apart (step=5)\n",
    "* reducing the step is similar to reducing the learning curve. \n",
    "* Increasing the length of each element of sequence is similar effect as to increase the batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DseSknmVSjtb",
    "outputId": "ea74ce9f-34a6-422c-b96c-09daf90c104f"
   },
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "# iterate through each character of the text to create a list \n",
    "# of ineteger numbers corresponding to each character\n",
    "encoded = [char_int[c] for c in text]\n",
    "sequences = [] # list of sequences each 40 char long\n",
    "next_char = [] # Next character (target) for each sequence\n",
    "\n",
    "\"\"\" \n",
    "The last \"i\" index may use up all the last 40 characters or leave\n",
    "some of them unused depending on what is the last index i. The unused \n",
    "portion if any, is discarded.\n",
    "\"\"\"\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    # next_char refers to the character encoding right after the last character of the sequence in encoded list\n",
    "    next_char.append(encoded[i + maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 891910, 178374)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[-1]), len(sequences[-2]), len(encoded), len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bH5I2EdPkZ6r",
    "outputId": "7deb382c-6325-447e-a7f3-712e5bf6fcfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96, 96)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 40 is the next char after the first sequence. Also it's 40th char or 35th char in 2nd sequence\n",
    "encoded[40], sequences[1][35], next_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "k0kXG75PGIci",
    "outputId": "7d2b379b-c0a0-455b-e93b-73214a810ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 104, 117, 7, 92, 31, 52, 79, 7, 31]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first char of the first article is C\n",
    "print(int_char[encoded[0]])\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fePaAxnVnexK",
    "outputId": "3e1fc5de-f7a1-4a64-8cd0-3f036c10c38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891910 891910\n"
     ]
    }
   ],
   "source": [
    "# One ineteger for each char\n",
    "print(len(text), len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ve5OyG_zSjth",
    "outputId": "ff9fec01-72c2-43d2-b3a5-b074478f51a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 [9, 104, 117, 7, 92, 31, 52, 79, 7, 31, 117, 36, 98, 118, 104, 71, 79, 18, 117, 31, 111, 7, 88, 88, 85, 86, 90, 98, 54, 104, 79, 111, 90, 98, 31, 111, 98, 104, 117, 98]\n"
     ]
    }
   ],
   "source": [
    "# each sequence is 40 in length\n",
    "print(len(sequences[0]), sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHXmmVcKaVtr"
   },
   "source": [
    "#### Create X and y\n",
    "* Save the sequence number, character codes in each sequence and the character location in each sequence in a boolian matrix\n",
    "* Save the next character code after each sequence as an index pointer in a boolian matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences), maxlen, len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUoyd1nySjtk"
   },
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "# Padding! initialize everything with False\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "char_indices = []\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        # x[sequence#[0:178374], char index in sequence[0:40], char encoded value[0:121]]\n",
    "        # in the last dimension of x only 40 out of 121 will be True\n",
    "        # 1 is stored as boolean type\n",
    "        x[i,t,char] = 1\n",
    "        # \n",
    "        char_indices.append((i,t,char))\n",
    "    # y[sequence, next character after sequence in embeded]   \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bMBX5AwXSjto",
    "outputId": "e692e588-986a-4cac-926a-4045c37c1c66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each sequence is on one row of axis=0, and will have boolean value(T/F)\n",
    "# (sequence#_index, char_loc_index, char_code_index)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rJH4cRcBSjtr",
    "outputId": "9095e221-44ce-44b5-9a24-25fcbc2edeff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (sequence#_index, char_code_index)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "xmVUMWndgTwx",
    "outputId": "ac1796a6-19ab-46af-fbe8-781fb90bb09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True (178373, 30, 110) d\n",
      "True (178373, 31, 98)  \n",
      "True (178373, 32, 7) t\n",
      "True (178373, 33, 86) h\n",
      "True (178373, 34, 92) r\n",
      "True (178373, 35, 90) e\n",
      "True (178373, 36, 90) e\n",
      "True (178373, 37, 98)  \n",
      "True (178373, 38, 40) p\n",
      "True (178373, 39, 90) e\n"
     ]
    }
   ],
   "source": [
    "# i is a tuple of indices that is for each character of each sequence in the flattened text\n",
    "# first 5 characters of the first sequence\n",
    "for i, tupidx in enumerate(char_indices[-10:]):\n",
    "  print(x[tupidx], char_indices[i-10], int_char[sequences[-1][i-10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 121)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen, len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `input_shape` parameter simply tells the input layer what the shape of one sample looks like.\n",
    "* If one sample of the input tensor only has one dimension – which is the case with one-dimensional / flattened arrays, in this case, you can also simply use `input_dim`: specifying the number of elements within that first dimension only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b91wThegSjtv"
   },
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "# x is the input sequence\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\"number of rows does not need to be defined in \n",
    "input_shape as it's related to batch size and it's dynamic\n",
    "\"\"\"\n",
    "# It's a sequence of inputs than LSTM analyzes\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# y is the next character after the sequence\n",
    "model.add(Dense(units=len(chars), activation='softmax'))\n",
    "\n",
    "# It's more like a ohe than integer target hence cce is better suited thatn scce.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nnr7pu8q3Y4"
   },
   "source": [
    "* lstm paramters = 4*(n+m+1)*m\n",
    "    - n=input dimension=len(chars)=121. For a given sample maxlen says that only 40 of the 121 inputs are True\n",
    "\n",
    "* m=# of lstm in layer, \n",
    "* dense layer (output) parameters = (128 parallel lstm + 1 bias)* 121 character set or output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ar77LC3qto-",
    "outputId": "cc201c20-f709-499b-e93e-06a6e268dcb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 15609)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*(121+128+1)*128, (128+1)*121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "aRH_nrjjoKsV",
    "outputId": "342e8035-9d0d-4152-889e-96ccd3fa9c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               128000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 121)               15609     \n",
      "=================================================================\n",
      "Total params: 143,609\n",
      "Trainable params: 143,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFIHVwXmSjt1"
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    \"\"\"\n",
    "    It normalizes the array of preds to proba (with their sum equal to 1)\n",
    "    Then it picks the index of the first maximum based on a random draw with the proba array weight\n",
    "    \"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Null operation\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize to the sum of one for the array of probabilities\n",
    "    # exp_preds=preds is an array\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    #multinomial(# of experiments, list of probability of each outcome, number of output repititions)\n",
    "    # len(probas)=len(preds): showing how many times each possibility was selected\n",
    "    probas = np.random.multinomial(1, preds, 1)   #ex/ probas=[0,0,1,0,0]\n",
    "    \n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kMPoUe0-MBCu",
    "outputId": "dcc965ef-378d-441f-dad3-f699575bd225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(1, [1/3]*3, 2)\n",
    "ss = np.random.multinomial(1, [1/6, 1/3, 1/2], 1)\n",
    "print(ss)\n",
    "np.argmax(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xJL0gA0Sjt6"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    # Random prompt in order to grab a 40 consecutive character sample as the seed\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        # 400 is the length of generated text\n",
    "        # create a quary sequence:\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        # Predict the next character after the sequence\n",
    "        # preds is an array of length 121 \n",
    "        # with numbers between 0 and 1 \n",
    "        #corresponding to the strength of the prediction of each of 121 characters\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        # update the seed by moving one character forward\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print(\"\\n**********\\n\", generated)\n",
    "    print()\n",
    "    print(\"\\n\\nlast character of this epoch preds:\\n\", len(preds), type(preds))\n",
    "    print(preds)\n",
    "\n",
    "# After each epoch generates a brand new 400 characters out of a 40 character seed\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yoJKLihxSjt9",
    "outputId": "b4bb17ad-34dc-4c97-a8cd-7bdd0e399a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5565/5575 [============================>.] - ETA: 0s - loss: 1.7404\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"a Super Bowl hangover. They’re more conc\"\n",
      "a Super Bowl hangover. They’re more concreatly to shople that the ins “all factmyy, you have and so excepteds to state encearh empirlatical artimoss opan and just that hose which Elambain Dalolins Biless sny Asloa and Ukirigary Pott only presidents: known of desiwn had on the thetterves witn nises is who sag to aly not back for at the iss conflenings surges foriage that extitial Kassingly wabrist, and Trump stagomed afferent after a fac\n",
      "**********\n",
      " a Super Bowl hangover. They’re more concreatly to shople that the ins “all factmyy, you have and so excepteds to state encearh empirlatical artimoss opan and just that hose which Elambain Dalolins Biless sny Asloa and Ukirigary Pott only presidents: known of desiwn had on the thetterves witn nises is who sag to aly not back for at the iss conflenings surges foriage that extitial Kassingly wabrist, and Trump stagomed afferent after a fac\n",
      "\n",
      "\n",
      "\n",
      "last character of this epoch preds:\n",
      " 121 <class 'numpy.ndarray'>\n",
      "[1.58518390e-06 2.78256550e-11 1.20427546e-06 1.61212430e-08\n",
      " 1.56071007e-01 7.58296181e-08 6.51143864e-03 8.52655212e-04\n",
      " 5.86305537e-09 2.48869333e-06 2.31312851e-11 1.54018664e-09\n",
      " 2.81597783e-11 9.26617626e-03 7.63251240e-09 1.00526172e-07\n",
      " 7.05950640e-07 1.31317601e-01 4.71209471e-09 2.05915876e-05\n",
      " 7.67938269e-10 9.67767306e-08 3.68084140e-11 5.13614751e-08\n",
      " 1.06271841e-01 2.45025771e-11 2.28249780e-11 1.86845159e-06\n",
      " 3.44725521e-07 3.66764073e-03 7.23791871e-09 9.29117689e-08\n",
      " 2.22094428e-07 3.18965562e-07 4.93642129e-02 4.49196477e-06\n",
      " 3.23984173e-11 6.33190796e-02 4.50185965e-11 1.36830238e-07\n",
      " 2.30144862e-11 1.04828977e-07 2.80471046e-11 2.55491635e-11\n",
      " 1.10696385e-06 1.04186229e-05 4.81240932e-06 1.64585456e-10\n",
      " 2.28163898e-01 3.85819504e-07 8.08166988e-07 3.64497104e-12\n",
      " 7.15794158e-07 7.25933257e-03 2.97881337e-03 1.20164936e-08\n",
      " 1.92741756e-09 6.60845068e-09 5.75558943e-07 8.77545592e-09\n",
      " 1.02930744e-06 2.91505603e-05 3.03856039e-04 3.28439711e-07\n",
      " 2.99299960e-11 5.83229678e-08 2.02185717e-07 1.05026754e-06\n",
      " 2.77618994e-03 2.60649222e-06 4.30696902e-11 3.20337179e-08\n",
      " 1.94726020e-01 3.46759878e-11 1.30311004e-04 2.64956737e-11\n",
      " 1.40064635e-11 2.36075066e-04 3.11432383e-03 5.20717822e-06\n",
      " 6.07981576e-07 4.54398942e-05 4.49629049e-11 2.90807187e-11\n",
      " 8.32968539e-08 6.89847512e-10 3.57252389e-07 2.76411782e-09\n",
      " 3.15008436e-11 8.59920192e-07 3.07232767e-07 2.37098157e-05\n",
      " 1.53290311e-10 3.97509803e-11 7.42500333e-07 1.71378311e-02\n",
      " 3.82622716e-08 5.82991390e-07 9.17728432e-03 1.26400543e-08\n",
      " 3.02190308e-08 3.15926035e-11 2.88675143e-11 5.39289999e-07\n",
      " 7.90647988e-04 3.62572337e-08 7.10823826e-08 5.80443036e-08\n",
      " 1.03631442e-06 3.29355737e-10 9.67180995e-07 9.36042303e-12\n",
      " 1.05720159e-04 5.16708521e-03 3.70679209e-06 1.51465628e-07\n",
      " 2.48292636e-11 1.10272050e-03 1.28527056e-08 1.87574912e-07\n",
      " 1.56605965e-05]\n",
      "5575/5575 [==============================] - 37s 7ms/step - loss: 1.7407\n",
      "Epoch 2/2\n",
      "5567/5575 [============================>.] - ETA: 0s - loss: 1.7085\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"or promotions at any time. All Print Pro\"\n",
      "or promotions at any time. All Print Pronatiesa if des inter offered to ture anyerary jaugiewhed erecuped honws” couser his cale or harply in O puried the realeving for any deals for the prusicusion, ight affectige.\n",
      "\n",
      "Bitco and out a count, for vimering in the news. Ithisted adroused could that heptting it a fighali draw a trains to winnes a grombaries — is in ispressive. He sble unner will milial interviver her to Amisiding tould by rec\n",
      "**********\n",
      " or promotions at any time. All Print Pronatiesa if des inter offered to ture anyerary jaugiewhed erecuped honws” couser his cale or harply in O puried the realeving for any deals for the prusicusion, ight affectige.\n",
      "\n",
      "Bitco and out a count, for vimering in the news. Ithisted adroused could that heptting it a fighali draw a trains to winnes a grombaries — is in ispressive. He sble unner will milial interviver her to Amisiding tould by rec\n",
      "\n",
      "\n",
      "\n",
      "last character of this epoch preds:\n",
      " 121 <class 'numpy.ndarray'>\n",
      "[3.8159953e-08 6.7221839e-10 1.0725017e-07 1.8708357e-06 1.7936760e-01\n",
      " 9.3846796e-07 2.9048981e-04 9.0886303e-04 6.6725686e-10 7.3723786e-04\n",
      " 4.9921978e-10 4.1643016e-06 5.9322158e-10 6.0821013e-03 9.6226076e-09\n",
      " 2.4605455e-07 2.0497985e-06 3.4917216e-04 6.6177614e-07 2.0020423e-04\n",
      " 1.2910478e-07 3.5895562e-07 4.6648807e-10 4.2896801e-05 1.2159132e-02\n",
      " 6.3253830e-10 6.0940009e-10 4.2464394e-06 8.8128962e-07 1.4846553e-01\n",
      " 1.9716659e-07 5.8545356e-07 1.4695230e-06 4.0665648e-07 2.8505810e-02\n",
      " 1.0543750e-04 8.3400514e-10 8.1729785e-02 2.2624836e-06 1.6079789e-06\n",
      " 4.4594783e-10 1.7776865e-05 3.4569395e-10 5.7084071e-10 3.0446261e-06\n",
      " 2.6741452e-06 7.8273791e-07 3.4757957e-08 1.4830685e-02 4.9049681e-06\n",
      " 5.0004319e-06 6.7244079e-12 7.9483300e-07 8.7222559e-03 4.9684923e-03\n",
      " 7.1482118e-06 1.8102904e-10 3.5492153e-07 1.6013786e-06 4.6656310e-09\n",
      " 1.8927279e-08 3.3631442e-09 7.8132011e-02 8.1473136e-06 5.5770649e-10\n",
      " 5.8514433e-07 2.4909352e-06 6.4118860e-05 5.4482916e-05 2.4911230e-06\n",
      " 7.9366380e-10 1.0448131e-06 2.5014234e-01 6.5464439e-10 7.4364286e-04\n",
      " 6.4521821e-10 7.4221610e-12 1.5033644e-03 1.5889802e-04 1.5758481e-04\n",
      " 3.0435796e-05 3.2299526e-05 5.9587407e-10 4.6690296e-10 2.3843512e-07\n",
      " 2.8575004e-08 1.7839710e-06 2.7923638e-06 6.2380279e-10 2.9955972e-06\n",
      " 2.2540273e-05 3.9439991e-02 4.0475290e-09 5.1424215e-10 2.6333796e-06\n",
      " 4.4062264e-02 2.4034978e-06 1.9927149e-05 1.2895590e-02 2.5693837e-06\n",
      " 8.6976974e-07 3.7597650e-10 8.0685597e-10 2.5698677e-07 5.1068645e-02\n",
      " 2.2187558e-05 5.2702757e-07 8.4098059e-07 5.8509945e-06 3.9233736e-09\n",
      " 8.9743016e-06 1.9223670e-10 1.6186545e-02 6.3724662e-03 5.4458683e-06\n",
      " 7.5734465e-06 7.9466506e-10 6.1508210e-04 4.6736044e-08 3.1139364e-06\n",
      " 1.0679830e-02]\n",
      "5575/5575 [==============================] - 37s 7ms/step - loss: 1.7083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff265f4a470>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the models\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vh4ElzDSjuB"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rys0_j8CSjuD"
   },
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LS_DS17_431_RNN_and_LSTM_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML_ANN (python3.7)",
   "language": "python",
   "name": "ml_ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
