{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skhabiri/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/LS_DS17_431_RNN_and_LSTM_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7QXzrvrSjru"
   },
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_5DZJg0Sjrw"
   },
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - **understand how data varies over time (or any sequential order), and use the order/time dimension predictively.**\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "**A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\"** - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)\n",
    "**RNN stands for Recurrent Neural Network. An LSTM is a Long Short-Term Memory layer, which is a type of RNN.\n",
    "These layers, built into a network, function as some sort of memory that allows the network to infer from not only the present, but also past events. They do this by keeping a sort of “internal memory” that they modify to keep track of meaningful events that happened.\n",
    "So really, an LSTM network is a quite common type of RNN. It has been widely adopted since the original RNN layer would tend overwrite too much of its “internal memory” at each step, losing the ability to infer from events further in past. The LSTM layer architecture is instead built in such a way the the network “decides” whether to modify its “internal memory” at each step. Doing so, and if properly trained, the layer can keep track of important events from further in the past, allowing for much richer inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fkzy3-FDSjr1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. **The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.**\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved. $C_{t-1}$ can be chosen to pass through to $C_{t+1}$\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - **language is inherently ordered data (letters/words go one after another, and the order *matters*).** [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h-vcbT3r4M3"
   },
   "source": [
    "* In the above diagram the path from input sequence of Xt-1, X, Xt+1 to purple lstm cells and to pink output neurons are all parallel (the vertical path in diagram) while lstm cells they do interact with each other in series manner (the horizental path). The horizental path could be bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RgdZYqBadli6",
    "outputId": "6b7aa5a6-c01d-4bae-a604-3b5cfdcd16b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vanishing gradients similar to vanishing the history of sequence in future predictions\n",
    "0.0001 ** 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer Architecture:\n",
    "In this diagram our input sequences have `S` time steps, and each time step has `C` features. For each time step we have a LSTM cell that include `D` parallel hidden units. The input tensor to the LSTM layer is `[batch_size, step_times, features]`. The output tensor of the LSTM has `[batch_size, units]`. Time steps are in series and do not show in the output interface, or generate any parameter to train.\n",
    "\n",
    "<img src=\"https://github.com/skhabiri/ML-DeepLearning/raw/main/images/lstm_arch.png\"  alt=\"lstm\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKrg7cMKSjr8"
   },
   "source": [
    "# RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9IYScGvBT1g"
   },
   "source": [
    "## Steps for text setiment classification:\n",
    "* Read 25K imbd reviews, with a bag of 20K words as our dictionary of words >> x_train\n",
    "* each word is represented by an ordinal integer index\n",
    "* use sequence.pad_sequences to fix the size of each review to 80 words\n",
    "* In building the model use tf.keras.layers.Embedding layer to learn the spacial location of a dense vector of size 128 for each word in 20K dictionary in the context of all the reviews. The embedding would minimize the euclidean distance of the similar words in 128 dimensions. The embedding layer initializes the weights and through the back propagation from LSTM layer the weights learn the context. The embedding layer is a blueprint that provides 128 output dimensions based on 20K dictionary size (features) in input sequences with time steps =80.\n",
    "* LSTM layer puts the words into the context as a sequence of time steps = 80. Based on the training label it learns the relationship of the words and back propagates the loss gradient to train the weights through the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "e72c00f4-5218-4a61-e877-2f44c3535eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000,) train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "\"\"\" num_features is number of the most frequent words \n",
    "collected in the bag of words. The associated number to \n",
    "each word is an integer ordinal index.\n",
    "\"\"\"\n",
    "num_features = 20000\n",
    "\n",
    "# desired number of words per review\n",
    "timesteps = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_features)\n",
    "\n",
    "# 25000 reviews\n",
    "print(x_train.shape, 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0]\n",
      "218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670,     2,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167,     2,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4,     2,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(y_train[:10])\n",
    "print(len(x_train[0]))\n",
    "np.asarray(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19999)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word ids are assigned in a sequence from 1 to 19999\n",
    "min([min(i) for i in x_train]), max([max(i) for i in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "JHSOnf3ASjsJ",
    "outputId": "212f6e2e-b8e6-4a5c-b3f2-82a1d9c9498d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     4, ..., 19997, 19998, 19999])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each review is a list of word ids from 20K bag of words. \n",
    "# Numbers are assigned to words as an ordinal integer\n",
    "unique_words = np.unique(np.concatenate(x_train))\n",
    "print(len(unique_words))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QpMv7Ha8iSDZ",
    "outputId": "f435369b-c5ad-4121-f4cf-32710df0419b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (25000,)\n",
      "x_test shape:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Prep-padding shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Xmm5Q6BkiW-o",
    "outputId": "aa89ae21-71d3-4048-9f6c-e8566a00fe21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in first review:  218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4       15\n",
       "16      11\n",
       "5        9\n",
       "12       6\n",
       "22       6\n",
       "        ..\n",
       "92       1\n",
       "224      1\n",
       "100      1\n",
       "3941     1\n",
       "98       1\n",
       "Length: 123, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# repeated words in each review\n",
    "r0 = pd.Series(x_train[0]).value_counts()\n",
    "print(\"# of words in first review: \",r0.sum())\n",
    "r0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K_Uu_xSUibaY",
    "outputId": "eea7bc9c-4e66-40da-9733-1f74bd61487c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in x_train[:10]]  # num of words in the first ten reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "zFHhoTSAio7u",
    "outputId": "8d4a839b-1981-415e-880d-a04386bfeb24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 778,\n",
       " 128,\n",
       " 74,\n",
       " 12,\n",
       " 630,\n",
       " 163,\n",
       " 15,\n",
       " 4,\n",
       " 1766,\n",
       " 7982,\n",
       " 1051,\n",
       " 2,\n",
       " 32,\n",
       " 85,\n",
       " 156,\n",
       " 45,\n",
       " 40,\n",
       " 148,\n",
       " 139,\n",
       " 121,\n",
       " 664,\n",
       " 665,\n",
       " 10,\n",
       " 10,\n",
       " 1361,\n",
       " 173,\n",
       " 4,\n",
       " 749,\n",
       " 2,\n",
       " 16,\n",
       " 3804,\n",
       " 8,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 12,\n",
       " 43,\n",
       " 127,\n",
       " 24,\n",
       " 15344,\n",
       " 10,\n",
       " 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6th review is short\n",
    "x_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.preprocessing.sequence()`:\n",
    "> This function transforms a list (of length `num_samples`) of sequences into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
    "`num_timesteps` is either the `maxlen` argument if provided, or the length of the longest sequence in the list.\n",
    "* Signature:\n",
    "```\n",
    "sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ic0jzRvzSjsQ",
    "outputId": "ba12b679-86cb-4ce5-f950-f63fd81550fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "#.pad_sequence method truncates or pads from the beginning\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=timesteps)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=timesteps)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "rwUsR-PwwS1Q",
    "outputId": "671c1a75-b804-4277-8901-76ed73b3abbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,   778,   128,    74,    12,   630,   163,    15,\n",
       "           4,  1766,  7982,  1051,     2,    32,    85,   156,    45,\n",
       "          40,   148,   139,   121,   664,   665,    10,    10,  1361,\n",
       "         173,     4,   749,     2,    16,  3804,     8,     4,   226,\n",
       "          65,    12,    43,   127,    24, 15344,    10,    10],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMvE12pjx-4v"
   },
   "source": [
    "### 1. Word Embedding\n",
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "\n",
    "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, **in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "The position of a word in the learned vector space is referred to as its embedding.\n",
    "Two popular examples of methods of learning word embeddings from text include: Word2Vec. GloVe.\n",
    "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.**\n",
    "\n",
    "### 2. Keras Embedding Layer\n",
    "**Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.**\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "* `input_dim`: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* `output_dim`: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* `input_length`: This is the dimension of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 80)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "MHDK11zsSjsh",
    "outputId": "d546cdcb-5c03-4978-a86b-f432e46f858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# we are predicting the imdb review sentiment (output size =1)\n",
    "\n",
    "\"\"\"\n",
    "Embedding Layer:\n",
    "input_dim: size of dictionary\n",
    "output_dim: size of embedding vector for each word in teh sequence\n",
    "input_length: number of words in one sequence, # of time steps\n",
    "\n",
    "a sequence is a combination of 20K words.\n",
    "we need 20k*128=256k parameters to embed each sequence\n",
    "\"\"\"\n",
    "\n",
    "model.add(Embedding(input_dim= num_features, output_dim = 128, input_length=timesteps))\n",
    "\n",
    "# alternatively: dynamically takes the input shape of 80\n",
    "# model.add(Embedding(num_features, 128))\n",
    "\n",
    "# LSTM input: 128 + 1(bias)\n",
    "# LSTM output: 128 (arbitrary), timestep = 80, features=128\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# output is a binary classification representing the sentiment of the review\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# in Output Shape column (None, 80, 128) is (batch_size, #of words to embed, embedding vector size(128))\n",
    "# The most left None in Shape column is the dynamic batch_size for all the layers\n",
    "# the shape with None is dynamic and can be inferred\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131584"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM params = 4 * ((size_of_input + 1) * size_of_output + size_of_output^2)\n",
    "4*((128+1)*128+128**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "fgC8gsgYSjso",
    "outputId": "2b019884-bd09-4d66-a7ef-3d26d90122b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 98s 122ms/step - loss: 0.4975 - accuracy: 0.7398 - val_loss: 0.3657 - val_accuracy: 0.8375\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 0.2456 - accuracy: 0.9037 - val_loss: 0.4008 - val_accuracy: 0.8302\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 94s 120ms/step - loss: 0.1506 - accuracy: 0.9450 - val_loss: 0.5058 - val_accuracy: 0.8260\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 92s 118ms/step - loss: 0.0957 - accuracy: 0.9662 - val_loss: 0.5636 - val_accuracy: 0.8165\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.5928 - val_accuracy: 0.8165\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this ANN do?\n",
    "* Embedding layer: It takes an input of word sequence with length of 80 (# of time steps) and Bag of Word of 20K and converts it to a 3 dimension tensor of (batch_size, time_steps(80), features(20k)). It creates a blueprint network layer to output a tensor with 3 dimension (batch_size, time_stesp(80), embedding_dim(128)).\n",
    "\n",
    "* LSTM layer: The input tensor to the LSTM is (batch_size, time_steps(80), embedding_size(128)). LSTM evaluates the relationship between 80 words in the sequence with short/long term memory technique and allow back propagation to correct the weights throughout the entire network to contextualize the embeddings accordingly. time steps are connected in series and there is no paramter to adjust. However for parallel hidden units (128), input features (128) there ara paramters associated that can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4237242043018341,\n",
       "  0.252041757106781,\n",
       "  0.16468730568885803,\n",
       "  0.10760781913995743,\n",
       "  0.07354194670915604],\n",
       " 'accuracy': [0.8028799891471863,\n",
       "  0.8990399837493896,\n",
       "  0.9374799728393555,\n",
       "  0.9622399806976318,\n",
       "  0.973800003528595],\n",
       " 'val_loss': [0.36566397547721863,\n",
       "  0.4007636606693268,\n",
       "  0.5058155655860901,\n",
       "  0.563585638999939,\n",
       "  0.5927637219429016],\n",
       " 'val_accuracy': [0.8374800086021423,\n",
       "  0.8301600217819214,\n",
       "  0.8259999752044678,\n",
       "  0.8164799809455872,\n",
       "  0.8164799809455872]}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicorns.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "yfydL-ZRSjst",
    "outputId": "e13ec026-896e-46be-c0ce-09692bbbcd4e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **model.predict require an input of (batch_size=None, 80). Passing x_test[0] directly to .predict method will be considered (batch_size=80, 1). Hence we either pass input slice x_test[:1] or expand_dims with axis=0 to call the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (1, 80))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].shape, x_test[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     1,   591,   202,    14,    31,     6,\n",
       "         717,    10,    10, 18142, 10698,     5,     4,   360,     7,\n",
       "           4,   177,  5760,   394,   354,     4,   123,     9,  1035,\n",
       "        1035,  1035,    10,    10,    13,    92,   124,    89,   488,\n",
       "        7944,   100,    28,  1668,    14,    31,    23,    27,  7479,\n",
       "          29,   220,   468,     8,   124,    14,   286,   170,     8,\n",
       "         157,    46,     5,    27,   239,    16,   179, 15387,    38,\n",
       "          32,    25,  7944,   451,   202,    14,     6,   717],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing the batch_size dimension: 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13265833],\n",
       "       [0.65723205],\n",
       "       [0.40162528],\n",
       "       [0.5781395 ],\n",
       "       [0.6550277 ],\n",
       "       [0.57304525],\n",
       "       [0.325602  ],\n",
       "       [0.46977627],\n",
       "       [0.51240087],\n",
       "       [0.42681643]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrong predictions as the input is not correct format\n",
    "# This is a wrong way and takes 80 input features as batch_size=80\n",
    "print(\"missing the batch_size dimension:\",len(model.predict(x_test[0]))) \n",
    "model.predict(x_test[0])[-10:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QW4rh_-sGsos",
    "outputId": "e1ee5baa-c6aa-47f8-c3a8-9595b8af6c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24344578]]\n",
      "[[0.24344578]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.24344578]], dtype=float32)>,\n",
       " 0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query = np.expand_dims(a=x_test[0],axis=0)\n",
    "print(model.predict(query))\n",
    "print(model.predict(x_test[:1]))  \n",
    "\n",
    "# while the .predict(query) method returns a np.ndarray, model(query) returns a tensor\n",
    "model(x_test[:1]), y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVFa0NC7Sjs1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3RkqrGT1jRl"
   },
   "source": [
    "## Steps to generate Text:\n",
    "* Import articles. (`137` articles in 137 rows)\n",
    "* create one giant text of all the articles >> `text`\n",
    "* create a bag of unique characters with a corresponding unique integer >> `char, char_int, int_char`\n",
    "* create a list of interleaved sequences of the same length characters, `40` represented with integer keys of the characters as input data and a list of corresponding integer represented next_character (the charcater after the sequence) as the target label >> `sequences, next_char`\n",
    "> we can't train on this data as the next character *prediction* is a floating point that if it's rounded up or down, does not necessarily point to a predicted character as the integer representation of characters are random not ordinal\n",
    "* create multi dimensional boolean array for X: \n",
    "1. axis0: sequence number >> size `178374`\n",
    "2. axis1: position of the character in sequence >> size `40`\n",
    "3. axis2: identifier of the character in the bag of characters >> size `33`\n",
    "* create a multi dimensional boolean array for Y:\n",
    "1. axis0: sequence number >> size `178374`\n",
    "2. axis1: identifier of the next_char in the bag of characters >> size `33`\n",
    "* Build the LSTM model\n",
    "* define a callback at the end of each epoch:\n",
    "1. pick a random prompt (index) in the concatinated giant `text`\n",
    "2. grab the 40 characters of the `text` as the query seed for character generation\n",
    "3. convert the query seed to X.shape, i.e. (1, 40, 33) >> `x_pred`\n",
    "4. get a y prediction from model after each epoch training, shape: (1, 33) >> `preds` an array of 33 floating values beytween 0 and 1, 1 being the strongest possibility for being the next_char\n",
    "5. scale the values of y array to proba and take one *draw* from the array considering the value of proba. grab the selected char from draw and prints it as the next char after the sequence.\n",
    "6. shift the input sequence to the right by 1 and predict the next char again >> get `400` characters iteratively\n",
    "* This way we work with probability of all the characters as the next char instead of a floating number that is supposed to resemble one of the characters integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-kuJzHnSjs3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOWH0qNC6QuV"
   },
   "source": [
    "* The dataset consists of 136 new articles each article in one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "CB2EzxcmmWhE",
    "outputId": "46774a35-45d5-46f0-8a39-81590c56cb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When President Trump announced his decision to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Like an aging rock star, the president is now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article\n",
       "0    Contributing columnist\\n\\nThe House is on fire...\n",
       "1    When President Trump announced his decision to...\n",
       "10   Russian President Vladimir Putin speaks at a s...\n",
       "100  “The Queen’s Speech” is designed to acknowledg...\n",
       "101  Like an aging rock star, the president is now ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "    'https://raw.githubusercontent.com/skhabiri/ML-DeepLearning/main/data/wp_articles.json')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10-d1GUVSjtC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29.txt', '15.txt', '114.txt', '100.txt', '128.txt', '129.txt', '101.txt', '115.txt', '14.txt', '28.txt', '16.txt', '103.txt', '117.txt', '116.txt', '102.txt', '17.txt', '13.txt', '106.txt', '112.txt', '113.txt', '107.txt', '12.txt', '10.txt', '38.txt', '111.txt', '105.txt', '104.txt', '110.txt', '39.txt', '11.txt', '76.txt', '62.txt', '89.txt', '88.txt', '63.txt', '77.txt', '49.txt', '61.txt', '75.txt', '74.txt', '60.txt', '48.txt', '64.txt', '70.txt', '58.txt', '59.txt', '71.txt', '65.txt', '73.txt', '67.txt', '9.txt', '98.txt', '99.txt', '8.txt', '66.txt', '72.txt', '57.txt', '5.txt', '43.txt', '94.txt', '80.txt', '81.txt', '95.txt', '42.txt', '56.txt', '4.txt', '68.txt', '40.txt', '6.txt', '54.txt', '83.txt', '97.txt', '96.txt', '82.txt', '7.txt', '55.txt', '41.txt', '69.txt', '45.txt', '51.txt', '3.txt', '79.txt', '86.txt', '92.txt', '93.txt', '87.txt', '78.txt', '50.txt', '2.txt', '44.txt', '0.txt', '52.txt', '46.txt', '91.txt', '85.txt', '84.txt', '90.txt', '47.txt', '1.txt', '53.txt', '34.txt', '20.txt', '135.txt', '121.txt', '109.txt', '108.txt', '120.txt', '134.txt', '21.txt', '35.txt', '23.txt', '37.txt', '122.txt', '123.txt', '36.txt', '22.txt', '26.txt', '32.txt', '127.txt', '133.txt', '132.txt', '126.txt', '33.txt', '27.txt', '31.txt', '25.txt', '19.txt', '118.txt', '130.txt', '124.txt', '125.txt', '131.txt', '119.txt', '18.txt', '24.txt', '30.txt']\n",
      "\n",
      "number of articles: 136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, we'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you're running locally:\n",
    "\n",
    "data_files = os.listdir('../data/articles')  \n",
    "print(data_files)\n",
    "\n",
    "# Read in Data\n",
    "data1 = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'../data/articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data1.append(f.read())\n",
    "\n",
    "print(\"\\nnumber of articles:\", len(data1))\n",
    "data1[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "H-Rw-vNT5ImV",
    "outputId": "32e0a516-dcc6-49c2-fd80-efcbce3c1e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin speaks at a summit in Turkmenistan on Friday. (Alexei Druzhinin/Sputnik/Kremlin Pool/AP)\\n\\nWith Mariana Alfaro\\n\\nTHE BIG IDEA: Vladimir Putin has won so much these past three years that he may get tired of winning.\\n\\nThe U.S. intelligence community’s January 2017 report on Russian interference in the previous year’s presidential campaign sought to explain why Donald Trump was so attractive to Moscow. This sentence has fresh salience: “Pro-Kremlin proxy Vladimir Zhir'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 500 characters of the third article\n",
    "df['article'][df['article'].index[2]][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lL56kYH_SjtK",
    "outputId": "0d999f7d-b947-4cc8-9c3d-108210f2fba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas series into a ndarray of 136 string elements\n",
    "data = df['article'].values\n",
    "print(data.shape, type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "zYu5BO-_SjtQ",
    "outputId": "8150e5c4-57c5-4ce2-fcb2-945a85ad42d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin speaks at a summit in Turkmenistan on Friday. (Alexei Druzhinin/Sputnik/Kremlin Pool/AP)\\n\\nWith Mariana Alfaro\\n\\nTHE BIG IDEA: Vladimir Putin has won so much these past three years that he may get tired of winning.\\n\\nThe U.S. intelligence community’s January 2017 report on Russian interference in the previous year’s presidential campaign sought to explain why Donald Trump was so attractive to Moscow. This sentence has fresh salience: “Pro-Kremlin proxy Vladimir Zhir'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data's implicit indices are reset\n",
    "data[2][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_MBOMMIOLAd"
   },
   "source": [
    "### Bag of characters in the form of a dictionary with corresponding integers assigned\n",
    "* Flatten the dataset into a giant string named text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75QPh0KzSjtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891910\n",
      "M B ] C Q © _ ⁩ I * d m Z ó s ― h 8 🤔 l . 7 ) & 4 , ⅓ 9 “ ’ u r 5 ( ⭐ A 0 D N o 2 Y • — T x ­ L j t X … E ö ' ã b z 3 i R ⁦ O ‘ 👻 \n",
      " f e 1   6 - 🗣 H \" ” ½ á G w [ ê g ⅔ $ P K / # ﬂ é í S W { k J q a @ ? è · ● ! U c V | : p – + y n v × % F ñ ;\n"
     ]
    }
   ],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "#.join is a string method. joining each row of the ndarray to another row with a white space\n",
    "# join iterables of data with a space\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters, enumerate(set(text)) would produce random indexing at every run\n",
    "# therefore make a list of the set\n",
    "\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "# list of characters\n",
    "print(\" \".join(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' clean up the training text and remove the characters that we don't want to show up in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873153\n",
      "i k q a o ? f e d m   x ! c s p j h l . y t n w g v , ' b z u r ;\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# remove non alphabet characters\n",
    "text = re.sub(r'[^a-zA-Z .,\\'!;?]', \" \", text)\n",
    "\n",
    "# lower case all characters\n",
    "text = text.lower()\n",
    "\n",
    "# strip the extra white spaces\n",
    "text = \" \".join(text.split())\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "# list of characters\n",
    "print(\" \".join(set(text)))\n",
    "\n",
    "chars = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup Tables \n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cO8fk5KU-hLc",
    "outputId": "24f81eb5-1304-4480-8ff9-d7b3e1769194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873153 <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'contributing columnist the house is on fire. and w'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text), type(text))\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZZ1UHyJuEO55",
    "outputId": "1067383c-ed61-478c-d5fd-b4f7fa360ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# number of characters\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qJW9phVonGhK",
    "outputId": "1274ff3e-5880-4909-a698-edb55bac1f59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_int[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ginZieymnNdY",
    "outputId": "d11d7319-c94c-485b-9bde-a5574aec7f44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_char[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO5qC5K-P-bG"
   },
   "source": [
    "### Generate the training sequences\n",
    "First create same length sequences of the data.\n",
    "* sample the character sequence of 40 (length of sequence) consecutive characters every 5 character apart (step=5)\n",
    "* reducing the step is similar to reducing the learning curve. \n",
    "* Increasing the length of each element of sequence is similar effect as to increase the batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DseSknmVSjtb",
    "outputId": "ea74ce9f-34a6-422c-b96c-09daf90c104f"
   },
   "outputs": [],
   "source": [
    "timesteps = 40\n",
    "step = 5\n",
    "\n",
    "# iterate through each character of the text to create a list \n",
    "# of ineteger numbers corresponding to each character\n",
    "encoded = [char_int[c] for c in text]\n",
    "sequences = [] # list of sequences each 40 char long\n",
    "next_char = [] # Next character (target) for each sequence\n",
    "\n",
    "\"\"\" \n",
    "The last \"i\" index may use up all the last 40 characters or leave\n",
    "some of them unused depending on what is the last index i. The unused \n",
    "portion if any, is discarded.\n",
    "\"\"\"\n",
    "for i in range(0, len(encoded) - timesteps, step):\n",
    "    sequences.append(encoded[i : i + timesteps])\n",
    "    # next_char refers to the character encoding right after the last character of the sequence in encoded list\n",
    "    next_char.append(encoded[i + timesteps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 873153, 174623)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[-1]), len(sequences[-2]), len(encoded), len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bH5I2EdPkZ6r",
    "outputId": "7deb382c-6325-447e-a7f3-712e5bf6fcfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 40 is the next char after the first sequence. Also it's 40th char or 35th char in 2nd sequence\n",
    "encoded[40], sequences[1][35], next_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "k0kXG75PGIci",
    "outputId": "7d2b379b-c0a0-455b-e93b-73214a810ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13, 4, 22, 21, 31, 0, 28, 30, 21, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first char of the first article is c\n",
    "print(int_char[encoded[0]])\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fePaAxnVnexK",
    "outputId": "3e1fc5de-f7a1-4a64-8cd0-3f036c10c38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873153 873153\n"
     ]
    }
   ],
   "source": [
    "# One ineteger for each char\n",
    "print(len(text), len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ve5OyG_zSjth",
    "outputId": "ff9fec01-72c2-43d2-b3a5-b074478f51a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 [13, 4, 22, 21, 31, 0, 28, 30, 21, 0, 22, 24, 10, 13, 4, 18, 30, 9, 22, 0, 14, 21, 10, 21, 17, 7, 10, 17, 4, 30, 14, 7, 10, 0, 14, 10, 4, 22, 10, 6]\n"
     ]
    }
   ],
   "source": [
    "# each sequence is 40 in length\n",
    "print(len(sequences[0]), sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHXmmVcKaVtr"
   },
   "source": [
    "#### Create X and y\n",
    "* Save the sequence number, character codes in each sequence and the character location in each sequence in a boolian matrix\n",
    "* Save the next character code after each sequence as an index pointer in a boolian matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174623, 40, 33)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences), timesteps, len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUoyd1nySjtk"
   },
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "# Padding! initialize everything with False\n",
    "x = np.zeros((len(sequences), timesteps, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "char_indices = []\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        # x[sequence#[0:178374], char index in sequence[0:40], char encoded value[0:33]]\n",
    "        # 1 is stored as boolean type\n",
    "        x[i,t,char] = 1\n",
    "        # \n",
    "        char_indices.append((i,t,char))\n",
    "    # y[sequence, next character after sequence in embeded]   \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bMBX5AwXSjto",
    "outputId": "e692e588-986a-4cac-926a-4045c37c1c66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174623, 40, 33)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each sequence is on one row of axis=0, and will have boolean value(T/F)\n",
    "# (sequence#_index, char_loc_index, char_code_index)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rJH4cRcBSjtr",
    "outputId": "9095e221-44ce-44b5-9a24-25fcbc2edeff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174623, 33)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (sequence#_index, char_code_index)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "xmVUMWndgTwx",
    "outputId": "ac1796a6-19ab-46af-fbe8-781fb90bb09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True (174622, 30, 21) t\n",
      "True (174622, 31, 17) h\n",
      "True (174622, 32, 31) r\n",
      "True (174622, 33, 7) e\n",
      "True (174622, 34, 7) e\n",
      "True (174622, 35, 10)  \n",
      "True (174622, 36, 15) p\n",
      "True (174622, 37, 7) e\n",
      "True (174622, 38, 4) o\n",
      "True (174622, 39, 15) p\n"
     ]
    }
   ],
   "source": [
    "# i is a tuple of indices that is for each character of each sequence in the flattened text\n",
    "# first 5 characters of the first sequence\n",
    "for i, tupidx in enumerate(char_indices[-10:]):\n",
    "  print(x[tupidx], char_indices[i-10], int_char[sequences[-1][i-10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 33)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps, len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `input_shape` parameter simply tells the input layer what the shape of one sample looks like.\n",
    "* If one sample of the input tensor only has one dimension – which is the case with one-dimensional / flattened arrays, in this case, you can also simply use `input_dim`: specifying the number of elements within that first dimension only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nnr7pu8q3Y4"
   },
   "source": [
    "### LSTM input sequence:\n",
    "We have an input sequence of characters. So our sequence step size is 40. each charcater has 33 features (that all except one has False value) and we have total of 178374 sequences. So the LSTM tensor input has 40 time steps and each step has 33 features. We would also provide 128 parallel lstm hidden units in each LSTM cell that is connected to each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b91wThegSjtv"
   },
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "# x is the input sequence\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\"number of rows does not need to be defined in \n",
    "input_shape as it's related to batch size and it's dynamic\n",
    "\"\"\"\n",
    "# It's a sequence of inputs than LSTM analyzes\n",
    "model.add(LSTM(128, input_shape=(timesteps, len(chars))))\n",
    "# y is the next character after the sequence\n",
    "model.add(Dense(units=len(chars), activation='softmax'))\n",
    "\n",
    "# It's more like a ohe than integer target hence cce is better suited thatn scce.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter calculations:**\n",
    "* lstm paramters = 4*(n+m+1)*m\n",
    "    - n=input features= 33.\n",
    "    - m = # of units = 128\n",
    "    - time steps = 40 (in series with no paramter to train\n",
    "\n",
    "* dense layer (output) parameters = (128 parallel lstm units + 1 bias)* 33 dense neurons at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ar77LC3qto-",
    "outputId": "cc201c20-f709-499b-e93e-06a6e268dcb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82944, 4257)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*(33+128+1)*128, (128+1)*33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "aRH_nrjjoKsV",
    "outputId": "342e8035-9d0d-4152-889e-96ccd3fa9c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               82944     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 33)                4257      \n",
      "=================================================================\n",
      "Total params: 87,201\n",
      "Trainable params: 87,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0043016091968684"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It napierian log\n",
    "np.log(2.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kMPoUe0-MBCu",
    "outputId": "dcc965ef-378d-441f-dad3-f699575bd225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([37, 26, 37]), 100), (array([38, 35, 27]), 100)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutput: [33, 37, 30]: \\ninterpretation: for the three classes out of 100 experiments:\\n    - 33 times class0 was sampled\\n    - 37 times class1 was sampled\\n    - 30 times class2 was sampled\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "multinomial(# of experiments(draws), list of probability of each outcome, number of output repititions)\n",
    "\"\"\"\n",
    "\n",
    "# return two sets of output that tell in 100 experiments how many of each of those 3 classes were selected\n",
    "zz = np.random.multinomial(100, [1/3]*3, 2)\n",
    "print([(output, sum(output)) for output in zz])\n",
    "\n",
    "# In one experiment which class is picked\n",
    "ss = np.random.multinomial(1, [1/6, 1/3, 1/2], 1)\n",
    "ss, np.argmax(ss)\n",
    "\n",
    "\"\"\"\n",
    "output: [33, 37, 30]: \n",
    "interpretation: for the three classes out of 100 experiments:\n",
    "    - 33 times class0 was sampled\n",
    "    - 37 times class1 was sampled\n",
    "    - 30 times class2 was sampled\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFIHVwXmSjt1"
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    \"\"\"\n",
    "    It normalizes the array of preds to proba (with their sum equal to 1)\n",
    "    Then it returns the index of the sampled character prediction based on \n",
    "    a random draw with the proba array weight\n",
    "    \"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Null operation\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize to the sum of one to represent probabilities\n",
    "    # exp_preds is an array\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    #multinomial(# of experiments(draws), list of probability of each outcome, number of output repititions)\n",
    "    # len(probas)=len(preds): showing how many times each possibility was selected\n",
    "    probas = np.random.multinomial(1, preds, 1)   #ex/ probas=[0,0,1,0,0]\n",
    "    \n",
    "    # Returns the index of the sampled value (count=1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xJL0gA0Sjt6"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print(\"logs.keys): \", logs.keys)\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    # Random prompt in order to grab a 40 consecutive character sample as the seed\n",
    "    start_index = random.randint(0, len(text) - timesteps - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + timesteps]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        # 400 is the length of generated text\n",
    "        \n",
    "        # create a quary sequence:\n",
    "        x_pred = np.zeros((1, timesteps, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        # Predict the next character after the sequence\n",
    "        # preds is an array of length 33 \n",
    "        # with numbers between 0 and 1 \n",
    "        #corresponding to the strength of the prediction of each of 33 characters\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        # update the seed by moving one character forward\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        \n",
    "        # forces the buffer to \"flush\", meaning that it will write everything \n",
    "        # in the buffer to the terminal, even if normally it would wait before doing so.\n",
    "        sys.stdout.flush()\n",
    "    print(\"\\n**********\\n\")\n",
    "    print()\n",
    "\n",
    "# After each epoch generates a brand new 400 characters out of a 40 character seed\n",
    "# LambdaCallback passes (epoch, logs) to the function defined by on_epoch_end here\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yoJKLihxSjt9",
    "outputId": "b4bb17ad-34dc-4c97-a8cd-7bdd0e399a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5457/5457 [==============================] - 146s 26ms/step - loss: 2.4947\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb021fff510>\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ed t.c. williams high school in alexandr\"\n",
      "ed t.c. williams high school in alexandre tuome a youbin tyne sand fother tay of int leabrabalk to kent teisald und onhles seondicann a fcomfily hiskiles of denow, iesenseclica ave intidace ti tat blime trom ons dendatital wose savicg to anehar ervaretley kiilt buthd ios. insanter haging aid as and the torcaus no has age out has irsing s urdec there orpanmare thas or as, enden a hoxe megipuetent the donn conturnty a ous in theo. ad anda\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "5457/5457 [==============================] - 111s 20ms/step - loss: 2.0049\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb0192284c8>\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"ct any errors or omissions in the servic\"\n",
      "ct any errors or omissions in the servicempic amyenthtict dy. the sclacemy nos mann them has any it she gool elatugre of by a ract thes feammer and chadizensuints, and crak in lish freme pray, jb.. sougd allaziess dequant rack us comuly poljok bent ongion frot a hamborts ovec. you the past beenecy ak ad jouse amoup the publens, a pridencires porterers from moth smibstrumans and reaad soft in id r stcon werd a mishung you po.t at ussed a\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "5457/5457 [==============================] - 103s 19ms/step - loss: 1.8619\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb0202fe948>\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"oward journalists that it spawns videos \"\n",
      "oward journalists that it spawns videos deal redilroa wor from unt. and ove stawignt helad of the woundda was intergints oppleachment of anomerong. to a fourdgry nown those ors. bect of is the laxies clic incoumiustonessong the being seok. in in the pray or as we spending cuater; woll. as lom of e.shies tas a ressuges whese in the huck. mis higgoning in indow. in figtionation wo hees, preptianse in hargal he bat fougnals, wolk wote, eri\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "5457/5457 [==============================] - 103s 19ms/step - loss: 1.7592\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb02538be58>\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"e elderly. there seemed to be little in \"\n",
      "e elderly. there seemed to be little in the iniwle and s proposted withmelined in ogions, jor trats. whiarend retein a myso desed in is great in rumbicted in the ander gnoule about to they leaso state, treap in jupt darling and not housh s meloug the wallinsso of a pspaces apport the geal kiver part ov remay, say! day be repis videry decenter that get repurennes, prs. fleperamer. the comortedment. in the uhengille to they gen worden tha\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "5457/5457 [==============================] - 101s 18ms/step - loss: 1.6842\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb025126630>\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ger in alaska. how to catch a mole and f\"\n",
      "ger in alaska. how to catch a mole and from their sound transid act stiender chood lock. by an fance are for most there jost alloked triz the said read becoundured wheme the new disonawly will carr. in such as comenorical ascowners was gan that to a playces tare trump some nottnings alomad, the more, dealling. of fired this serteem on compain a was s rea taid accoance, hilling a defays they trancout, by shore even re severage for eegrai\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "5457/5457 [==============================] - 97s 18ms/step - loss: 1.6206\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb021ff4fc0>\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"on is really stuck a decade or two behin\"\n",
      "on is really stuck a decade or two behinited in carces lood age to the pughment you voo pobleral retolle for includitians ressive at the gailed polico facereud this to dusticall, man privanalier prican with relietian video, surcesting. sait ow sownet earal and protection, but with those funds but cay sex effly, advisuol bill, but diventions of aisprite transofformements as the also ince.m. studges addicable known kinstack aggeen. i mad \n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "5457/5457 [==============================] - 94s 17ms/step - loss: 1.5732\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb02308a168>\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"ands, like the lucayan, to work the fiel\"\n",
      "ands, like the lucayan, to work the field on mayhe scomposia s garian kiff nimed browing consenteverys. he had rellate, as prolected unionbe to aday, sayer deal wide durters undelige is youk to reaton stad the ever dwing dearlage own. . declout manker cullen for use, who then after dway again. have troughed the violanncan sublitty, frakes. or retions, breaply to the elan about the losed that is usails, the crudings wild righters on the \n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "5457/5457 [==============================] - 95s 17ms/step - loss: 1.5312\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb02351f048>\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"irst american leader to turn his back on\"\n",
      "irst american leader to turn his back on a anowam s moters to was could have president likes, compan with a sowed the ush sumities are she begran i s unchurri and papplit of beight least sip. turks in videos. the tomered they case deticm. kresmonss to contect with umiestand of violeg, accest visit on a sintiration cake information infagesting filly ovtreatly reseive ted to loods where him wren gail afternom and morvectional from houghty\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "5457/5457 [==============================] - 94s 17ms/step - loss: 1.4886\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb022f6df30>\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \" with the kurds against iraq, who we jus\"\n",
      " with the kurds against iraq, who we jusk, the so defime assapboring a round with perpless has frore dors, in from fookually had worn devesding to subsitted for services were after the distribity to moliticuul only life hew octivally , phoble, agp told by the not fort, or juding differex natessimie intored inats jooked use to pro uppooking weaton, the esso a kilj maron leading at the umors to product, who wrate buons in ourged sustard a\n",
      "**********\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "5457/5457 [==============================] - 94s 17ms/step - loss: 1.4606\n",
      "logs.keys):  <built-in method keys of dict object at 0x7fb03294ee10>\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"pensive is a rowhouse in need of substan\"\n",
      "pensive is a rowhouse in need of substanting drays aftires restarrs, in the troadrep polits of cyses devenion. and only know oper whater a staritatimiles. brocked to an ad, accoud nearly belaid shart headld of hear roon to and wexen, javinj and campria play but medications we house and the startionated rad m. trump s been released the offices. the and i american and interview of the accumed in manbing toosy from your commettion con, plo\n",
      "**********\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e10279b7e873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# fit the models\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rys0_j8CSjuD"
   },
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LS_DS17_431_RNN_and_LSTM_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML_ANN (python3.7)",
   "language": "python",
   "name": "ml_ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
