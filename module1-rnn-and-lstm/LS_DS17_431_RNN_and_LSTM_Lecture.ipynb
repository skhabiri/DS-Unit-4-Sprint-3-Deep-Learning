{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS17_431_RNN_and_LSTM_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhabiri/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/LS_DS17_431_RNN_and_LSTM_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ldr0HZ193GKb"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 1*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7QXzrvrSjru",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_5DZJg0Sjrw",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "# Neural Networks for Sequences (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fkzy3-FDSjr1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgdZYqBadli6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a8e4ad3-e22d-44f4-988b-1e9768070a9d"
      },
      "source": [
        "# Vanishing gradients\n",
        "0.0001 ** 1000"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TKrg7cMKSjr8"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "03b506ba-e53f-4eb6-c8aa-e77cc6d95857"
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\"\"\" max_features is number of the most frequent words \n",
        "collected in the bag of words. The associated number to \n",
        "each word refers to the count of that word in the dataset.\n",
        "\"\"\"\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(x_train.shape, 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "# 25000 reviews"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "(25000,) train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHSOnf3ASjsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e58198b5-9575-4674-a724-a1d2ac85ee1e"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 2]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpMv7Ha8iSDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "abf6f54d-1d1f-4a15-9981-864c36e4ddad"
      },
      "source": [
        "# Prep-padding shape\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape:  (25000,)\n",
            "x_test shape:  (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmm5Q6BkiW-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "75fed46b-ac57-42df-f7f4-5074628675df"
      },
      "source": [
        "import pandas as pd\n",
        "# repeated words in each review\n",
        "pd.Series(x_train[0]).value_counts()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4       15\n",
              "16      11\n",
              "5        9\n",
              "12       6\n",
              "22       6\n",
              "        ..\n",
              "92       1\n",
              "224      1\n",
              "100      1\n",
              "3941     1\n",
              "98       1\n",
              "Length: 123, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Uu_xSUibaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82fbe020-1f96-4e04-bd56-1de98b9e660f"
      },
      "source": [
        "[len(x) for x in x_train[:10]]  # num of words from bag of 20K words in first ten reviews"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFHhoTSAio7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "ec3891f3-b5e8-477c-b221-4087da87f74f"
      },
      "source": [
        "# 5th review is short\n",
        "x_train[5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 778,\n",
              " 128,\n",
              " 74,\n",
              " 12,\n",
              " 630,\n",
              " 163,\n",
              " 15,\n",
              " 4,\n",
              " 1766,\n",
              " 7982,\n",
              " 1051,\n",
              " 2,\n",
              " 32,\n",
              " 85,\n",
              " 156,\n",
              " 45,\n",
              " 40,\n",
              " 148,\n",
              " 139,\n",
              " 121,\n",
              " 664,\n",
              " 665,\n",
              " 10,\n",
              " 10,\n",
              " 1361,\n",
              " 173,\n",
              " 4,\n",
              " 749,\n",
              " 2,\n",
              " 16,\n",
              " 3804,\n",
              " 8,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 12,\n",
              " 43,\n",
              " 127,\n",
              " 24,\n",
              " 15344,\n",
              " 10,\n",
              " 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic0jzRvzSjsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bca4506c-8f7c-4aa0-b9a8-0ddf0061247c"
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "#pad_sequence truncates/pads from the beginning\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad Sequences (samples x time)\n",
            "x_train shape:  (25000, 80)\n",
            "x_test shape:  (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwUsR-PwwS1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "578166cb-2307-46c9-a17d-280d3ac31bbe"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
              "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
              "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
              "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
              "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
              "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
              "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
              "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
              "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRxQrGTCSjsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2c27903d-5a92-44a7-f63c-c4dd44cf1dff"
      },
      "source": [
        "x_train[5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     1,   778,   128,    74,    12,   630,   163,    15,\n",
              "           4,  1766,  7982,  1051,     2,    32,    85,   156,    45,\n",
              "          40,   148,   139,   121,   664,   665,    10,    10,  1361,\n",
              "         173,     4,   749,     2,    16,  3804,     8,     4,   226,\n",
              "          65,    12,    43,   127,    24, 15344,    10,    10],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHDK11zsSjsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "259a2f5e-ce18-40db-a435-3ff6303504b4"
      },
      "source": [
        "model = Sequential()\n",
        "# we are predicting the imdb review sentiment (output =1)\n",
        "\n",
        "# convert 20K inputs to 128 embeddings (2560K params)\n",
        "model.add(Embedding(max_features, 128))\n",
        "# https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout\n",
        "# not related to the above 128 embedding\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgC8gsgYSjso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1648a78e-c64f-4cb2-b236-a53a95374e8b"
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=2, \n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 216s 276ms/step - loss: 0.4313 - accuracy: 0.7969 - val_loss: 0.3688 - val_accuracy: 0.8432\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 215s 275ms/step - loss: 0.2615 - accuracy: 0.8959 - val_loss: 0.3713 - val_accuracy: 0.8402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfydL-ZRSjst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4caf3eec-9304-4605-e618-738131a387e5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1QYAX5Sjsz",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "# LSTM Text generation with Keras (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVFa0NC7Sjs1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-kuJzHnSjs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOWH0qNC6QuV",
        "colab_type": "text"
      },
      "source": [
        "* The dataset consists of 136 new articles each article in one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB2EzxcmmWhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "191c1560-9d8e-4c12-899d-d29a7b405ae2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/main/module1-rnn-and-lstm/wp_articles.json')\n",
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When President Trump announced his decision to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>Like an aging rock star, the president is now ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article\n",
              "0    Contributing columnist\\n\\nThe House is on fire...\n",
              "1    When President Trump announced his decision to...\n",
              "10   Russian President Vladimir Putin speaks at a s...\n",
              "100  “The Queen’s Speech” is designed to acknowledg...\n",
              "101  Like an aging rock star, the president is now ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdhtXdOG4hoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9ca162ef-0431-4864-b8c9-088d89e7435c"
      },
      "source": [
        "print(df.shape)\n",
        "df.iloc[0,:]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "article    Contributing columnist\\n\\nThe House is on fire...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uoj5TAaSjs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_files = os.listdir('./articles')  # If you're running locally"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Rw-vNT5ImV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "924918f7-428e-44d3-949f-d159c8c4526c"
      },
      "source": [
        "df['article'][0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, went on to complete his presidency with high approval ratings and has remained a popular former president.\\n\\nIf you care about democracy, the rule of law and nearly 250 years of constitutional governance, take heed. President Trump is no Clinton or Nixon, or even Johnson. He will not go quietly. It will be ugly. He will betray us and the rule of law in the process — defying subpoenas, withholding documents, blocking witnesses.\\n\\nThis presidency is fouled with disrespect for rules, boundaries and norms. Trump walked away from major agreements negotiated by his predecessors — the Iran nuclear deal, the Paris climate accord — and the United States’ word as bond is no more. Look at the ease with which he discards supporters — ask former attorney general Jeff Sessions or former secretary of state Rex Tillerson. Ask our allies, here today, gone tomorrow — NATO, the Kurds in Syria.\\n\\nFrom his earliest days as a candidate, Trump voiced appallingly arrogant views about the power of the presidency: “Mexico will pay for the wall!” ; “I alone can fix it”; “My primary consultant is myself.” His possessiveness over people and institutions is also not new: “my generals and my military,” “my African American.”\\n\\nOnly months into his presidency, Trump disparaged democratic allies, including Germany’s Angela Merkel (“ruining Germany”) and Britain’s Theresa May (“foolish”) — notably, both women — in favor of strong-arm leaders such as North Korea’s Kim Jong Un (who wrote him “beautiful letters”), Saudi Arabia’s Mohammed bin Salman (“very good ally”), Turkey’s Recep Tayyip Erdogan (“great friendship”) and the Philippines’ Rodrigo Duterte (“great relationship”). Trump heaps praise on Russia’s Vladimir Putin (“he’s a strong leader”). And, days after revealing his words pressuring Ukrainian President Volodymyr Zelensky to dig up dirt on his opponent, he invited China to do it, too.\\n\\nTrump’s campaign for the White House was rotten from the beginning. We glimpsed its depths when his lawyer Michael Cohen pleaded guilty to campaign finance felonies and identified Trump as “Individual 1” in a conspiracy to pay off an adult-film star and a former Playboy model to silence them during the height of the 2016 presidential campaign. We got even more evidence of Trump’s deception in the dense report prepared by special counsel Robert S. Mueller III on Russia’s interference in the 2016 election to benefit Trump and try to defeat Hillary Clinton. Mueller laid the groundwork for at least 10 acts of obstruction of justice.\\n\\nEven with all of that, it’s this still-unraveling Ukraine story that makes clear the bits and pieces that we could only imagine with Trump’s pleas to “Russia, if you’re listening . . . .” We have the same threats, lies, subterfuge and obstruction — only this time, we have the president’s unambiguous words to Zelensky: “I would like you to do us a favor though.” Ukraine represents the same lawlessness that propelled Trump over the finish line in 2016: this time in plain sight, with witnesses, including at least one whistleblower and lots of bit players. From the State Department to the Energy Department to the Justice Department and throughout the White House, Trump is using every bit of the machinery of government and personnel at his disposal to strongarm a small country under the heel of its threatening Russian neighbor — all to get manufactured dirt on a political opponent.\\n\\nIt’s illegal. The evidence is bearing fruit. The time will come. And justice will be served.\\n\\nThe president’s personal approval rating remains low, though stable, but there is growing support for impeachment — a Fox News poll this week found that 51 percent support removing Trump from office. Independents, as well as Democrats, mostly support the impeachment inquiry, while Republicans are mostly holding tight. These things may or may not change.\\n\\nEither way, we will be changed if we do not right this ship of democracy.\\n\\n“Impeachment is not about punishment. Impeachment is about cleansing the office. Impeachment is about restoring honor and integrity to the office.” We should heed these words, spoken by the 1999 version of Sen. Lindsey O. Graham (R-S.C.). The fire did not start with Ukraine. Nonetheless, Ukraine may give us the water to finally put it out.\\n\\nRead more from Donna F. Edwards's archive.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10-d1GUVSjtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Read in Data\n",
        "\n",
        "# data = []\n",
        "\n",
        "# for file in data_files:\n",
        "#     if file[-3:] == 'txt':\n",
        "#         with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
        "#             data.append(f.read())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL56kYH_SjtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9768de3d-4a82-457f-d27c-b0fc3c98f211"
      },
      "source": [
        "data = df['article'].values\n",
        "print(data.shape, type(data))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136,) <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYu5BO-_SjtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "fb0d5cf5-6094-44d0-af66-2d557e59fd5c"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, went on to complete his presidency with high approval ratings and has remained a popular former president.\\n\\nIf you care about democracy, the rule of law and nearly 250 years of constitutional governance, take heed. President Trump is no Clinton or Nixon, or even Johnson. He will not go quietly. It will be ugly. He will betray us and the rule of law in the process — defying subpoenas, withholding documents, blocking witnesses.\\n\\nThis presidency is fouled with disrespect for rules, boundaries and norms. Trump walked away from major agreements negotiated by his predecessors — the Iran nuclear deal, the Paris climate accord — and the United States’ word as bond is no more. Look at the ease with which he discards supporters — ask former attorney general Jeff Sessions or former secretary of state Rex Tillerson. Ask our allies, here today, gone tomorrow — NATO, the Kurds in Syria.\\n\\nFrom his earliest days as a candidate, Trump voiced appallingly arrogant views about the power of the presidency: “Mexico will pay for the wall!” ; “I alone can fix it”; “My primary consultant is myself.” His possessiveness over people and institutions is also not new: “my generals and my military,” “my African American.”\\n\\nOnly months into his presidency, Trump disparaged democratic allies, including Germany’s Angela Merkel (“ruining Germany”) and Britain’s Theresa May (“foolish”) — notably, both women — in favor of strong-arm leaders such as North Korea’s Kim Jong Un (who wrote him “beautiful letters”), Saudi Arabia’s Mohammed bin Salman (“very good ally”), Turkey’s Recep Tayyip Erdogan (“great friendship”) and the Philippines’ Rodrigo Duterte (“great relationship”). Trump heaps praise on Russia’s Vladimir Putin (“he’s a strong leader”). And, days after revealing his words pressuring Ukrainian President Volodymyr Zelensky to dig up dirt on his opponent, he invited China to do it, too.\\n\\nTrump’s campaign for the White House was rotten from the beginning. We glimpsed its depths when his lawyer Michael Cohen pleaded guilty to campaign finance felonies and identified Trump as “Individual 1” in a conspiracy to pay off an adult-film star and a former Playboy model to silence them during the height of the 2016 presidential campaign. We got even more evidence of Trump’s deception in the dense report prepared by special counsel Robert S. Mueller III on Russia’s interference in the 2016 election to benefit Trump and try to defeat Hillary Clinton. Mueller laid the groundwork for at least 10 acts of obstruction of justice.\\n\\nEven with all of that, it’s this still-unraveling Ukraine story that makes clear the bits and pieces that we could only imagine with Trump’s pleas to “Russia, if you’re listening . . . .” We have the same threats, lies, subterfuge and obstruction — only this time, we have the president’s unambiguous words to Zelensky: “I would like you to do us a favor though.” Ukraine represents the same lawlessness that propelled Trump over the finish line in 2016: this time in plain sight, with witnesses, including at least one whistleblower and lots of bit players. From the State Department to the Energy Department to the Justice Department and throughout the White House, Trump is using every bit of the machinery of government and personnel at his disposal to strongarm a small country under the heel of its threatening Russian neighbor — all to get manufactured dirt on a political opponent.\\n\\nIt’s illegal. The evidence is bearing fruit. The time will come. And justice will be served.\\n\\nThe president’s personal approval rating remains low, though stable, but there is growing support for impeachment — a Fox News poll this week found that 51 percent support removing Trump from office. Independents, as well as Democrats, mostly support the impeachment inquiry, while Republicans are mostly holding tight. These things may or may not change.\\n\\nEither way, we will be changed if we do not right this ship of democracy.\\n\\n“Impeachment is not about punishment. Impeachment is about cleansing the office. Impeachment is about restoring honor and integrity to the office.” We should heed these words, spoken by the 1999 version of Sen. Lindsey O. Graham (R-S.C.). The fire did not start with Ukraine. Nonetheless, Ukraine may give us the water to finally put it out.\\n\\nRead more from Donna F. Edwards's archive.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75QPh0KzSjtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "#.join is a string method. joining each row of the ndarray to another row with a white space\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters, enumerate(set(text)) would produce random indexing at every run\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables \n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wmmFY5ODT0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2f8d78e-e1f8-41b6-ccdb-7fa672427850"
      },
      "source": [
        "set(text)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '{',\n",
              " '|',\n",
              " '©',\n",
              " '\\xad',\n",
              " '·',\n",
              " '½',\n",
              " '×',\n",
              " 'á',\n",
              " 'ã',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'í',\n",
              " 'ñ',\n",
              " 'ó',\n",
              " 'ö',\n",
              " '–',\n",
              " '—',\n",
              " '―',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '•',\n",
              " '…',\n",
              " '\\u2066',\n",
              " '\\u2069',\n",
              " '⅓',\n",
              " '⅔',\n",
              " '●',\n",
              " '⭐',\n",
              " 'ﬂ',\n",
              " '👻',\n",
              " '🗣',\n",
              " '🤔'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO8fk5KU-hLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0c5cc462-e114-44b1-dcc3-3eb3173b8ba6"
      },
      "source": [
        "print(len(text), type(text))\n",
        "text[:50]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891910 <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Contributing columnist\\n\\nThe House is on fire. And '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ1UHyJuEO55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd68faa7-5a51-436c-dca4-2a3f25d290a9"
      },
      "source": [
        "print(len(chars))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJW9phVonGhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f2b6995-0fa1-479c-f1dc-6a4738c369d0"
      },
      "source": [
        "char_int[' ']"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ginZieymnNdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67b1c3b3-8fc4-41ba-8524-6565115793ea"
      },
      "source": [
        "int_char[57]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'🤔'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfVb3B2LSjtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfdb8568-11bd-49bb-f619-147364fb3a79"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DseSknmVSjtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "939d3d9c-0c46-4b04-b179-f2fcec49db8d"
      },
      "source": [
        "# Create the sequence data\n",
        "# sample the character sequence of 40 characters every 5 character\n",
        "# reducing the step is similar to reducing the learning curve. \n",
        "# Increasing the length of each element of sequence is similar to increasing the batch_size\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "# iterate through each character of the text\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One character for each element of sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    # next_char refers to the character encoding right after the last character of the sequence in encoded list\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences), len(encoded)/step)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  178374 178382.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH5I2EdPkZ6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27394885-4a19-477a-ca95-66a1e434d7ed"
      },
      "source": [
        "encoded[40], sequences[1][35], next_char[0]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(91, 91, 91)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0kXG75PGIci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "61899a86-c0bd-43cc-83af-86851782159a"
      },
      "source": [
        "print(int_char[encoded[0]])\n",
        "encoded[:10]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[92, 79, 112, 90, 83, 74, 26, 96, 90, 74]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePaAxnVnexK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe74f38a-0de5-48df-8be6-2bd84ab2078b"
      },
      "source": [
        "print(len(text), len(encoded))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891910 891910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5OyG_zSjth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60ef760e-d402-450a-ba23-3140de71bcad"
      },
      "source": [
        "print(len(sequences[0]), sequences[0])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40 [92, 79, 112, 90, 83, 74, 26, 96, 90, 74, 112, 48, 46, 77, 79, 110, 96, 47, 112, 74, 93, 90, 99, 99, 13, 64, 87, 46, 100, 79, 96, 93, 87, 46, 74, 93, 46, 79, 112, 46]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYx5X-gEkDuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOABiAqjjTKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c87502d-c692-4c6f-9f3e-7b85538f7be8"
      },
      "source": [
        "print()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUoyd1nySjtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "# Padding!\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "test = []\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        # x[sequence#[0:137], char index in sequence[0:40], char encoded value[0:121]]\n",
        "        # 1 is stored as boolean type\n",
        "        x[i,t,char] = 1\n",
        "        test.append((i,t,char))\n",
        "    # y[sequence, next character after sequence in embeded]   \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMBX5AwXSjto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "874cfb31-336b-488e-887e-c7ec6bae1f32"
      },
      "source": [
        "# each sequence is on one row of axis=0, and will have boolean value(T/F)\n",
        "x.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 40, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJH4cRcBSjtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ae52805-05f8-4e17-a27a-d3d4e1602148"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVUMWndgTwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e82712f9-b93b-4daa-90a7-7d2789b64ec3"
      },
      "source": [
        "for i in test[:5]:\n",
        "  print(x[i])\n",
        "test[-5:]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(178373, 35, 87),\n",
              " (178373, 36, 87),\n",
              " (178373, 37, 46),\n",
              " (178373, 38, 44),\n",
              " (178373, 39, 87)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvyQa-xmv0z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91wThegSjtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "# x is the input sequence\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "# y is the output next character after the sequence\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ar77LC3qto-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a5967cf-bbc1-4f39-ebf0-724aa80fd32b"
      },
      "source": [
        "4*(121+128+1)*128"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnr7pu8q3Y4",
        "colab_type": "text"
      },
      "source": [
        "* n=input dimension=len(chars)=121, maxlen says that only 40 of the 121 inputs are True\n",
        "\n",
        "* m=# of lstm in layer, # paramters = 4*(n+m+1)*m\n",
        "* output parameters = (128 parallel lstm + 1 bias)* 121 (character set or output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRH_nrjjoKsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2fd5980d-de84-42f6-b83b-bf1c20b1ee41"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 128)               128000    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 121)               15609     \n",
            "=================================================================\n",
            "Total params: 143,609\n",
            "Trainable params: 143,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIHVwXmSjt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    \"\"\"\n",
        "    It normalizes the array of preds to proba (with their sum equal to 1)\n",
        "    Then it picks the index of the first maximum based on a random draw with the proba array weight\n",
        "    \"\"\"\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    \n",
        "    # Null operation\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    \n",
        "    # Normalize to the sum of one for the array of probabilities\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    \n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    \n",
        "    # Returns the indices of the maximum values along an axis.\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMPoUe0-MBCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a1eb104-262d-42c5-90d8-8660a97d43b3"
      },
      "source": [
        "np.random.multinomial(1, [1/3]*3, 2)\n",
        "ss = np.random.multinomial(1, [1/6, 1/3, 1/2], 1)\n",
        "print(ss)\n",
        "np.argmax(ss)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJL0gA0Sjt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    # Random prompt to grab a 40 character sample seed\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        # 400 is the length of generated text\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        # Predict the next step (character)\n",
        "        # preds is an array of length 121 \n",
        "        # with all the elements zero except only one of them 1\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        # update the seed by moving one character forward\n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoJKLihxSjt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6f2c2a-54ef-492c-cca5-adc084efd42f"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5575/5575 [==============================] - ETA: 0s - loss: 2.5548\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"ow while responding to a call about an o\"\n",
            "ow while responding to a call about an orz isllyTs go dbin 1n That loatiges spwatsole tians, Kare Tred cans lizs redap wit werkons whall siers, Aoe hed conkan Alcack, ajvit a dfut ij wa Ued the was las out ond an of irces equns. Mat Nan to Rsirntoces Meve$ santed Mamen. fo d d jiy, Laln sobop Hote Res in pimscos walr all erns ive’s, Jow ivpan hers treas.\n",
            "\n",
            "L\n",
            "\n",
            "Tha chat cayro a n)Maill in and -chopld, Guryade, un a dswend Fesle\n",
            "\n",
            "\n",
            "Thoud fur\n",
            "5575/5575 [==============================] - 254s 46ms/step - loss: 2.5548\n",
            "Epoch 2/10\n",
            "5575/5575 [==============================] - ETA: 0s - loss: 2.2140\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"l shows that during the exchange, Trump \"\n",
            "l shows that during the exchange, Trump a Cand Wa han tle sal sreedrarzipntwanco, im Butep PCaquippalk ford impo th of a cating annov.’ mand that to nolrabic, stodr, in raded mimoned “atarizastiffind. chichlU ary os be. git with m..nctema]vion to N4-Bep and traities, wall ow. “Amuce ta filtal’s ry at mest mill 191600” betled atnen’t douss of woll allisins ated net, con, torn and th aldrinationrrasegrte. The derecresed stopre offediout d\n",
            "5575/5575 [==============================] - 259s 47ms/step - loss: 2.2140\n",
            "Epoch 3/10\n",
            "5574/5575 [============================>.] - ETA: 0s - loss: 2.0762\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \" bad movies — in this case, the British \"\n",
            " bad movies — in this case, the British a forting of terstade. Dezarem uncander TharE, Krovereleaal ard Wisk an madtion. Lonahd sme turt’s corving desmors fish to eftrman of chouster firmsion to suill bele to nowive Trump a zolisal dising tratacry for owwersing a lycduter arony, tho ate. ow this meying to the copkents ay were to om sealer acrint af efole in ofraity to rution.\n",
            "\n",
            "“Tree Dillour Ne an oug oftiog and curging of lack extridal \n",
            "5575/5575 [==============================] - 264s 47ms/step - loss: 2.0762\n",
            "Epoch 4/10\n",
            "5574/5575 [============================>.] - ETA: 0s - loss: 1.9797\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"old, damp, and longing to go home.\n",
            "\n",
            "Ther\"\n",
            "old, damp, and longing to go home.\n",
            "\n",
            "There and to Whith kinu offerivical proan, all your prace “so Os wat fidento all’s praty adonors unary. “This the Cobpis, sorpries or 2006 reqoit,” sabse to Trump spatem that shovide the their Maloovy houthence of hy parneds the juticers on Recan Onvinoting Lo able Thenrsarsuak Cost)\n",
            "\n",
            "AD\n",
            "\n",
            "As Sescriatie signe. AD\n",
            "\n",
            "SIthers — ro indaatour Sa-chen Pupth theture’s light proctar oftings hid sh-o7 dequated s\n",
            "5575/5575 [==============================] - 263s 47ms/step - loss: 1.9797\n",
            "Epoch 5/10\n",
            "5574/5575 [============================>.] - ETA: 0s - loss: 1.9073\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"drawn to different scales, falsely imply\"\n",
            "drawn to different scales, falsely implyaing on twaing om Clecusity Mand Ampcolis saip thaids for in “Boe od in the et polo-s teraming lough exfornces ffor sumeces have oul amy last the simssigy are refugines from do suela’ly Denad. Sande. The easor hem it holsable gernorn that the peoching as the 2-mund he aamare burt know vive.\n",
            "\n",
            "AD 14 peole,.\n",
            "\n",
            "Stwinges 17k M/S. ORm as what you trung sheacres of ouj she crile this whech and on a ginter\n",
            "5575/5575 [==============================] - 266s 48ms/step - loss: 1.9073\n",
            "Epoch 6/10\n",
            "5574/5575 [============================>.] - ETA: 0s - loss: 1.8462\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \" space between them. I resurrect them in\"\n",
            " space between them. I resurrect them insepts fun esown for “I Ascessige etsure the wat Bot’s an the Holminal hee skeck.\n",
            "\n",
            "Mutteranon wares as in Tewss from non’t or be bost crabeen stegtions, he was tastovental improbely and gove out.\n",
            "\n",
            "Thruzanse-Chiven ”s mades: one sead caune weotes streadous this the sigen that chunneched ad huse meecle for the erecumperay ighones, geeds.\n",
            "\n",
            "AD\n",
            "\n",
            "a beweign’t at a plen whore pasts. They ham. All bathe. At\n",
            "5575/5575 [==============================] - 268s 48ms/step - loss: 1.8461\n",
            "Epoch 7/10\n",
            "2656/5575 [=============>................] - ETA: 2:09 - loss: 1.7990"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-ca472b800112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vh4ElzDSjuB",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use a Keras LSTM to generate text on today's assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rys0_j8CSjuD",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "    * Sequence Problems:\n",
        "        - Time Series (like Stock Prices, Weather, etc.)\n",
        "        - Text Classification\n",
        "        - Text Generation\n",
        "        - And many more! :D\n",
        "    * LSTMs are generally preferred over RNNs for most problems\n",
        "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
        "    * Keras has LSTMs/RNN layer types implemented nicely\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
        "    * Shape of input data is very important\n",
        "    * Can take a while to train\n",
        "    * You can use it to write movie scripts. :P "
      ]
    }
  ]
}