{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS17_431_RNN_and_LSTM_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhabiri/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/LS_DS17_431_RNN_and_LSTM_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ldr0HZ193GKb"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 1*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7QXzrvrSjru",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_5DZJg0Sjrw",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "# Neural Networks for Sequences (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fkzy3-FDSjr1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h-vcbT3r4M3",
        "colab_type": "text"
      },
      "source": [
        "* In the above diagram the path from input sequence of Xt-1, X, Xt+1 to purple lstm cells and to pink output neurons are all parallel (the vertical path in diagram) while lstm cells they do interact with each other in series manner (the horizental path). The horizental path could be bi-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgdZYqBadli6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b7aa5a6-c01d-4bae-a604-3b5cfdcd16b7"
      },
      "source": [
        "# Vanishing gradients similar to vanishing the history of sequence in future predictions\n",
        "0.0001 ** 1000"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TKrg7cMKSjr8"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9IYScGvBT1g",
        "colab_type": "text"
      },
      "source": [
        "## Steps for text setiment classification:\n",
        "* Read 25K imbd reviews, with a bag of 20K words as our dictionary of words >> `x_train`\n",
        "* each word is represented by its relative count in the entire dataset as its integer representation\n",
        "* use sequence.pad_sequences to fix the size of each review to 80 words\n",
        "* In building the model use tf.keras.layers.Embedding layer to learn the spacial location of a dense vector of size `128` for each word in 20K dictionary in the context of all the reviews.\n",
        "* Use LSTM layer to build up the model and train it based on the binary sentiment in y label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e72c00f4-5218-4a61-e877-2f44c3535eab"
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\"\"\" max_features is number of the most frequent words \n",
        "collected in the bag of words. The associated number to \n",
        "each word refers to the count of that word in the dataset.\n",
        "\"\"\"\n",
        "max_features = 20000\n",
        "\n",
        "# desired number of words per review\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(x_train.shape, 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "# 25000 reviews"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "(25000,) train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IZWh7xcuNCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35226494-7530-466e-a294-a760bce5d015"
      },
      "source": [
        "len(x_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHSOnf3ASjsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "212f6e2e-b8e6-4a5c-b3f2-82a1d9c9498d"
      },
      "source": [
        "# each review is a list of word ids from 20K bag of words. \n",
        "# The word id is the total count of the word in the entire dataset.\n",
        "x_train"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 2]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpMv7Ha8iSDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f435369b-c5ad-4121-f4cf-32710df0419b"
      },
      "source": [
        "# Prep-padding shape\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape:  (25000,)\n",
            "x_test shape:  (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmm5Q6BkiW-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "aa89ae21-71d3-4048-9f6c-e8566a00fe21"
      },
      "source": [
        "import pandas as pd\n",
        "# repeated words in each review\n",
        "pd.Series(x_train[0]).value_counts()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4       15\n",
              "16      11\n",
              "5        9\n",
              "12       6\n",
              "22       6\n",
              "        ..\n",
              "92       1\n",
              "224      1\n",
              "100      1\n",
              "3941     1\n",
              "98       1\n",
              "Length: 123, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Uu_xSUibaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eea7bc9c-4e66-40da-9733-1f74bd61487c"
      },
      "source": [
        "[len(x) for x in x_train[:10]]  # num of words from bag of 20K words in first ten reviews"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFHhoTSAio7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "8d4a839b-1981-415e-880d-a04386bfeb24"
      },
      "source": [
        "# 5th review is short\n",
        "x_train[5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 778,\n",
              " 128,\n",
              " 74,\n",
              " 12,\n",
              " 630,\n",
              " 163,\n",
              " 15,\n",
              " 4,\n",
              " 1766,\n",
              " 7982,\n",
              " 1051,\n",
              " 2,\n",
              " 32,\n",
              " 85,\n",
              " 156,\n",
              " 45,\n",
              " 40,\n",
              " 148,\n",
              " 139,\n",
              " 121,\n",
              " 664,\n",
              " 665,\n",
              " 10,\n",
              " 10,\n",
              " 1361,\n",
              " 173,\n",
              " 4,\n",
              " 749,\n",
              " 2,\n",
              " 16,\n",
              " 3804,\n",
              " 8,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 12,\n",
              " 43,\n",
              " 127,\n",
              " 24,\n",
              " 15344,\n",
              " 10,\n",
              " 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic0jzRvzSjsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ba12b679-86cb-4ce5-f950-f63fd81550fb"
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "#.pad_sequence method truncates or pads from the beginning\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad Sequences (samples x time)\n",
            "x_train shape:  (25000, 80)\n",
            "x_test shape:  (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwUsR-PwwS1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "671c1a75-b804-4277-8901-76ed73b3abbf"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
              "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
              "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
              "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
              "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
              "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
              "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
              "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
              "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRxQrGTCSjsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e8b868c2-899d-409e-ea8e-d53241a3f161"
      },
      "source": [
        "x_train[5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     1,   778,   128,    74,    12,   630,   163,    15,\n",
              "           4,  1766,  7982,  1051,     2,    32,    85,   156,    45,\n",
              "          40,   148,   139,   121,   664,   665,    10,    10,  1361,\n",
              "         173,     4,   749,     2,    16,  3804,     8,     4,   226,\n",
              "          65,    12,    43,   127,    24, 15344,    10,    10],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbCOuW5D7eGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc9af022-c1d0-4ac6-a545-5c45050b6854"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMvE12pjx-4v",
        "colab_type": "text"
      },
      "source": [
        "### 1. Word Embedding\n",
        "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
        "\n",
        "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
        "\n",
        "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
        "\n",
        "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
        "\n",
        "The position of a word in the learned vector space is referred to as its embedding.\n",
        "\n",
        "Two popular examples of methods of learning word embeddings from text include:\n",
        "\n",
        "Word2Vec.\n",
        "GloVe.\n",
        "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.*\n",
        "\n",
        "### 2. Keras Embedding Layer\n",
        "Keras offers an Embedding layer that can be used for neural networks on text data.\n",
        "\n",
        "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
        "\n",
        "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
        "\n",
        "It is a flexible layer that can be used in a variety of ways, such as:\n",
        "\n",
        "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
        "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
        "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
        "\n",
        "It must specify 3 arguments:\n",
        "\n",
        "`input_dim`: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "`output_dim`: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
        "`input_length`: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHDK11zsSjsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d546cdcb-5c03-4978-a86b-f432e46f858a"
      },
      "source": [
        "model = Sequential()\n",
        "# we are predicting the imdb review sentiment (output size =1)\n",
        "\n",
        "# convert 20K inputs to 20K dense vector of size 128 (2560K params)\n",
        "# input_length is the same as time steps in one input. \n",
        "# input_dim is not the same as batch size and it's specific to embedding's size of dictionary\n",
        "model.add(Embedding(input_dim= max_features, output_dim = 128, input_length=maxlen))\n",
        "\n",
        "# alternatively: dinamically takes the input shape of 80\n",
        "# model.add(Embedding(max_features, 128))\n",
        "\n",
        "\n",
        "# not related to the above 128 embedding\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "# in Output Shape column (Nona, None, 128) is (batch_size, #of input features(80), dense vector size(128))\n",
        "# the shape with None is dynamic and can be inferred\n",
        "# Yhe most left None in Shape column is the dynamic batch_size for all the layers\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 80, 128)           2560000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgC8gsgYSjso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2b019884-bd09-4d66-a7ef-3d26d90122b1"
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=2, \n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 232s 297ms/step - loss: 0.4307 - accuracy: 0.7957 - val_loss: 0.3499 - val_accuracy: 0.8471\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 231s 295ms/step - loss: 0.2561 - accuracy: 0.8960 - val_loss: 0.4228 - val_accuracy: 0.8258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfydL-ZRSjst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e13ec026-896e-46be-c0ce-09692bbbcd4e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4rh_-sGsos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1ee5baa-c6aa-47f8-c3a8-9595b8af6c65"
      },
      "source": [
        "import numpy as np\n",
        "\"\"\"Since it's a dynamic shape passing x_test[0] directly \n",
        "to .predict method will be considered 80 inputs not 1. \n",
        "Hence we either pass input slice or expand_dims with axis=0 to call the model\"\"\"\n",
        "query = np.expand_dims(x_test[0],0)\n",
        "print(model.predict(query))\n",
        "print(model.predict(x_test[:1]))\n",
        "\n",
        "# This is a wrong way and takes 80 input features as batch_size=80\n",
        "print(\"missing the batch_size dimension:\",len(model.predict(x_test[0])))   \n",
        "\n",
        "# while the .predict method returns a np.ndarray, model(query) returns a tensor\n",
        "model(query)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.2411533]]\n",
            "[[0.2411533]]\n",
            "missing the batch_size dimension: 80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.2411533]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1QYAX5Sjsz",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "# LSTM Text generation with Keras (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVFa0NC7Sjs1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3RkqrGT1jRl",
        "colab_type": "text"
      },
      "source": [
        "## Steps to generate Text:\n",
        "* Import articles. (`137` articles in 137 rows)\n",
        "* create one giant text of all the articles >> `text`\n",
        "* create a bag of unique characters with a corresponding unique integer >> `char, char_int, int_char`\n",
        "* create a list of interleaved sequences of the same length characters, `40` represented with integer keys of the characters as input data and a list of corresponding integer represented next_character (the charcater after the sequence) as the target label >> `sequences, next_char`\n",
        "> we can't train on this data as the next character prediction is a floating point that if it's rounded up or down, does not necessarily point to a predicted character as the integer representation of characters are random\n",
        "* create multi dimensional boolean array for X: \n",
        "1. axis0: sequence number >> size `178374`\n",
        "2. axis1: position of the character in sequence >> size `40`\n",
        "3. axis2: identifier of the character in the bag of characters >> size `121`\n",
        "* create a multi dimensional boolean array for Y:\n",
        "1. axis0: sequence number >> size `178374`\n",
        "2. axis1: identifier of the next_char in the bag of characters >> size `121`\n",
        "* Build the LSTM model\n",
        "* define a callback at the end of each epoch:\n",
        "1. pick a random prompt (index) in the concatinated giant `text`\n",
        "2. grab the 40 characters of the `text` as the query seed for character generation\n",
        "3. convert the query seed to X.shape, i.e. (1, 40, 121) >> `x_pred`\n",
        "4. get a y prediction from model after each epoch training, shape: (1, 121) >> `preds` an array of 121 floating values beytween 0 and 1, 1 being the strongest possibility for being the next_char\n",
        "5. scale the values of y array to proba and take one draw from the array considering the value of proba. grab the selected char from draw and prints it as the next char after the sequence.\n",
        "6. shift the input sequence to the right by 1 and predict the next char again >> get `400` characters iteratively\n",
        "* This way we work with probability of all the characters as the next char instead of a floating number that is supposed to resemble one of the characters integer representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-kuJzHnSjs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOWH0qNC6QuV",
        "colab_type": "text"
      },
      "source": [
        "* The dataset consists of 136 new articles each article in one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB2EzxcmmWhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "46774a35-45d5-46f0-8a39-81590c56cb40"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/main/module1-rnn-and-lstm/wp_articles.json')\n",
        "df.head(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When President Trump announced his decision to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>Like an aging rock star, the president is now ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>The cause was cardiac arrest, said his longtim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>LeCroy had waited to do this. He spent the ear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>Updated January 9, 2017\\n\\nThis Terms of Sale ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>Updated June 4, 2018\\n\\nThis Terms of Sale gov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>“Eventually I woke up and looked at the score ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article\n",
              "0    Contributing columnist\\n\\nThe House is on fire...\n",
              "1    When President Trump announced his decision to...\n",
              "10   Russian President Vladimir Putin speaks at a s...\n",
              "100  “The Queen’s Speech” is designed to acknowledg...\n",
              "101  Like an aging rock star, the president is now ...\n",
              "102  The cause was cardiac arrest, said his longtim...\n",
              "103  LeCroy had waited to do this. He spent the ear...\n",
              "104  Updated January 9, 2017\\n\\nThis Terms of Sale ...\n",
              "105  Updated June 4, 2018\\n\\nThis Terms of Sale gov...\n",
              "106  “Eventually I woke up and looked at the score ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdhtXdOG4hoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "274a6f97-0c94-4a23-d6d7-870bf54d9ab4"
      },
      "source": [
        "print(df.shape)\n",
        "df.iloc[0,:]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "article    Contributing columnist\\n\\nThe House is on fire...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uoj5TAaSjs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_files = os.listdir('./articles')  # If you're running locally"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Rw-vNT5ImV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "32e0a516-dcc6-49c2-fd80-efcbce3c1e84"
      },
      "source": [
        "df['article'][0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, went on to complete his presidency with high approval ratings and has remained a popular former president.\\n\\nIf you care about democracy, the rule of law and nearly 250 years of constitutional governance, take heed. President Trump is no Clinton or Nixon, or even Johnson. He will not go quietly. It will be ugly. He will betray us and the rule of law in the process — defying subpoenas, withholding documents, blocking witnesses.\\n\\nThis presidency is fouled with disrespect for rules, boundaries and norms. Trump walked away from major agreements negotiated by his predecessors — the Iran nuclear deal, the Paris climate accord — and the United States’ word as bond is no more. Look at the ease with which he discards supporters — ask former attorney general Jeff Sessions or former secretary of state Rex Tillerson. Ask our allies, here today, gone tomorrow — NATO, the Kurds in Syria.\\n\\nFrom his earliest days as a candidate, Trump voiced appallingly arrogant views about the power of the presidency: “Mexico will pay for the wall!” ; “I alone can fix it”; “My primary consultant is myself.” His possessiveness over people and institutions is also not new: “my generals and my military,” “my African American.”\\n\\nOnly months into his presidency, Trump disparaged democratic allies, including Germany’s Angela Merkel (“ruining Germany”) and Britain’s Theresa May (“foolish”) — notably, both women — in favor of strong-arm leaders such as North Korea’s Kim Jong Un (who wrote him “beautiful letters”), Saudi Arabia’s Mohammed bin Salman (“very good ally”), Turkey’s Recep Tayyip Erdogan (“great friendship”) and the Philippines’ Rodrigo Duterte (“great relationship”). Trump heaps praise on Russia’s Vladimir Putin (“he’s a strong leader”). And, days after revealing his words pressuring Ukrainian President Volodymyr Zelensky to dig up dirt on his opponent, he invited China to do it, too.\\n\\nTrump’s campaign for the White House was rotten from the beginning. We glimpsed its depths when his lawyer Michael Cohen pleaded guilty to campaign finance felonies and identified Trump as “Individual 1” in a conspiracy to pay off an adult-film star and a former Playboy model to silence them during the height of the 2016 presidential campaign. We got even more evidence of Trump’s deception in the dense report prepared by special counsel Robert S. Mueller III on Russia’s interference in the 2016 election to benefit Trump and try to defeat Hillary Clinton. Mueller laid the groundwork for at least 10 acts of obstruction of justice.\\n\\nEven with all of that, it’s this still-unraveling Ukraine story that makes clear the bits and pieces that we could only imagine with Trump’s pleas to “Russia, if you’re listening . . . .” We have the same threats, lies, subterfuge and obstruction — only this time, we have the president’s unambiguous words to Zelensky: “I would like you to do us a favor though.” Ukraine represents the same lawlessness that propelled Trump over the finish line in 2016: this time in plain sight, with witnesses, including at least one whistleblower and lots of bit players. From the State Department to the Energy Department to the Justice Department and throughout the White House, Trump is using every bit of the machinery of government and personnel at his disposal to strongarm a small country under the heel of its threatening Russian neighbor — all to get manufactured dirt on a political opponent.\\n\\nIt’s illegal. The evidence is bearing fruit. The time will come. And justice will be served.\\n\\nThe president’s personal approval rating remains low, though stable, but there is growing support for impeachment — a Fox News poll this week found that 51 percent support removing Trump from office. Independents, as well as Democrats, mostly support the impeachment inquiry, while Republicans are mostly holding tight. These things may or may not change.\\n\\nEither way, we will be changed if we do not right this ship of democracy.\\n\\n“Impeachment is not about punishment. Impeachment is about cleansing the office. Impeachment is about restoring honor and integrity to the office.” We should heed these words, spoken by the 1999 version of Sen. Lindsey O. Graham (R-S.C.). The fire did not start with Ukraine. Nonetheless, Ukraine may give us the water to finally put it out.\\n\\nRead more from Donna F. Edwards's archive.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10-d1GUVSjtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Read in Data\n",
        "\n",
        "# data = []\n",
        "\n",
        "# for file in data_files:\n",
        "#     if file[-3:] == 'txt':\n",
        "#         with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
        "#             data.append(f.read())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL56kYH_SjtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d999f7d-b947-4cc8-9c3d-108210f2fba5"
      },
      "source": [
        "# Convert pandas series into a ndarray\n",
        "data = df['article'].values\n",
        "print(data.shape, type(data))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136,) <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYu5BO-_SjtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "8150e5c4-57c5-4ce2-fcb2-945a85ad42d1"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the office of the president. If only past defrocked presidents could provide a roadmap for this firestorm.\\n\\nAndrew Johnson fought impeachment vigorously and survived removal, but never won reelection. Richard Nixon got in the way of justice, but eventually bowed to the rule of law, accepting his asterisk in the annals of history and resigning before certain removal. Bill Clinton expressed contrition, went on to complete his presidency with high approval ratings and has remained a popular former president.\\n\\nIf you care about democracy, the rule of law and nearly 250 years of constitutional governance, take heed. President Trump is no Clinton or Nixon, or even Johnson. He will not go quietly. It will be ugly. He will betray us and the rule of law in the process — defying subpoenas, withholding documents, blocking witnesses.\\n\\nThis presidency is fouled with disrespect for rules, boundaries and norms. Trump walked away from major agreements negotiated by his predecessors — the Iran nuclear deal, the Paris climate accord — and the United States’ word as bond is no more. Look at the ease with which he discards supporters — ask former attorney general Jeff Sessions or former secretary of state Rex Tillerson. Ask our allies, here today, gone tomorrow — NATO, the Kurds in Syria.\\n\\nFrom his earliest days as a candidate, Trump voiced appallingly arrogant views about the power of the presidency: “Mexico will pay for the wall!” ; “I alone can fix it”; “My primary consultant is myself.” His possessiveness over people and institutions is also not new: “my generals and my military,” “my African American.”\\n\\nOnly months into his presidency, Trump disparaged democratic allies, including Germany’s Angela Merkel (“ruining Germany”) and Britain’s Theresa May (“foolish”) — notably, both women — in favor of strong-arm leaders such as North Korea’s Kim Jong Un (who wrote him “beautiful letters”), Saudi Arabia’s Mohammed bin Salman (“very good ally”), Turkey’s Recep Tayyip Erdogan (“great friendship”) and the Philippines’ Rodrigo Duterte (“great relationship”). Trump heaps praise on Russia’s Vladimir Putin (“he’s a strong leader”). And, days after revealing his words pressuring Ukrainian President Volodymyr Zelensky to dig up dirt on his opponent, he invited China to do it, too.\\n\\nTrump’s campaign for the White House was rotten from the beginning. We glimpsed its depths when his lawyer Michael Cohen pleaded guilty to campaign finance felonies and identified Trump as “Individual 1” in a conspiracy to pay off an adult-film star and a former Playboy model to silence them during the height of the 2016 presidential campaign. We got even more evidence of Trump’s deception in the dense report prepared by special counsel Robert S. Mueller III on Russia’s interference in the 2016 election to benefit Trump and try to defeat Hillary Clinton. Mueller laid the groundwork for at least 10 acts of obstruction of justice.\\n\\nEven with all of that, it’s this still-unraveling Ukraine story that makes clear the bits and pieces that we could only imagine with Trump’s pleas to “Russia, if you’re listening . . . .” We have the same threats, lies, subterfuge and obstruction — only this time, we have the president’s unambiguous words to Zelensky: “I would like you to do us a favor though.” Ukraine represents the same lawlessness that propelled Trump over the finish line in 2016: this time in plain sight, with witnesses, including at least one whistleblower and lots of bit players. From the State Department to the Energy Department to the Justice Department and throughout the White House, Trump is using every bit of the machinery of government and personnel at his disposal to strongarm a small country under the heel of its threatening Russian neighbor — all to get manufactured dirt on a political opponent.\\n\\nIt’s illegal. The evidence is bearing fruit. The time will come. And justice will be served.\\n\\nThe president’s personal approval rating remains low, though stable, but there is growing support for impeachment — a Fox News poll this week found that 51 percent support removing Trump from office. Independents, as well as Democrats, mostly support the impeachment inquiry, while Republicans are mostly holding tight. These things may or may not change.\\n\\nEither way, we will be changed if we do not right this ship of democracy.\\n\\n“Impeachment is not about punishment. Impeachment is about cleansing the office. Impeachment is about restoring honor and integrity to the office.” We should heed these words, spoken by the 1999 version of Sen. Lindsey O. Graham (R-S.C.). The fire did not start with Ukraine. Nonetheless, Ukraine may give us the water to finally put it out.\\n\\nRead more from Donna F. Edwards's archive.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_MBOMMIOLAd",
        "colab_type": "text"
      },
      "source": [
        "### Bag of characters in the form of a dictionary with corresponding integers assigned\n",
        "* Flatten the dataset into a giant string named text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75QPh0KzSjtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "#.join is a string method. joining each row of the ndarray to another row with a white space\n",
        "# join iterables of data with a space\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters, enumerate(set(text)) would produce random indexing at every run\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables \n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wmmFY5ODT0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28b7a9fe-02e9-4a75-f1b9-40a9b57b0809"
      },
      "source": [
        "set(text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '{',\n",
              " '|',\n",
              " '©',\n",
              " '\\xad',\n",
              " '·',\n",
              " '½',\n",
              " '×',\n",
              " 'á',\n",
              " 'ã',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'í',\n",
              " 'ñ',\n",
              " 'ó',\n",
              " 'ö',\n",
              " '–',\n",
              " '—',\n",
              " '―',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '•',\n",
              " '…',\n",
              " '\\u2066',\n",
              " '\\u2069',\n",
              " '⅓',\n",
              " '⅔',\n",
              " '●',\n",
              " '⭐',\n",
              " 'ﬂ',\n",
              " '👻',\n",
              " '🗣',\n",
              " '🤔'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO8fk5KU-hLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "24f81eb5-1304-4480-8ff9-d7b3e1769194"
      },
      "source": [
        "print(len(text), type(text))\n",
        "text[:50]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891910 <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Contributing columnist\\n\\nThe House is on fire. And '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ1UHyJuEO55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1067383c-ed61-478c-d5fd-b4f7fa360ec4"
      },
      "source": [
        "print(len(chars))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJW9phVonGhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1274ff3e-5880-4909-a698-edb55bac1f59"
      },
      "source": [
        "char_int[' ']"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ginZieymnNdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d11d7319-c94c-485b-9bde-a5574aec7f44"
      },
      "source": [
        "int_char[57]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‘'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO5qC5K-P-bG",
        "colab_type": "text"
      },
      "source": [
        "### Create same length sequences of the data\n",
        "* sample the character sequence of 40 (length of sequence) consecutive characters every 5 character apart (step=5)\n",
        "* reducing the step is similar to reducing the learning curve. \n",
        "* Increasing the length of each element of sequence is similar to increasing the batch_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DseSknmVSjtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea74ce9f-34a6-422c-b96c-09daf90c104f"
      },
      "source": [
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "# iterate through each character of the text to create a list \n",
        "# of ineteger numbers corresponding to each character\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # list of sequences each 40 char long\n",
        "next_char = [] # One character for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    # next_char refers to the character encoding right after the last character of the sequence in encoded list\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "# The following two numbers are comparable and close\n",
        "print('sequences: ', len(sequences), len(encoded)/step)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  178374 178382.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH5I2EdPkZ6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7deb382c-6325-447e-a7f3-712e5bf6fcfb"
      },
      "source": [
        "# 40 is the next char after the first sequence. Also it's 40th char or 35th char in 2nd sequence\n",
        "encoded[40], sequences[1][35], next_char[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104, 104, 104)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0kXG75PGIci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d2b379b-c0a0-455b-e93b-73214a810ea4"
      },
      "source": [
        "# first char of the first article is C\n",
        "print(int_char[encoded[0]])\n",
        "encoded[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50, 77, 24, 48, 17, 13, 54, 117, 48, 13]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePaAxnVnexK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e1fc5de-f7a1-4a64-8cd0-3f036c10c38e"
      },
      "source": [
        "# One ineteger for each char\n",
        "print(len(text), len(encoded))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891910 891910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5OyG_zSjth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ff9fec01-72c2-43d2-b3a5-b074478f51a1"
      },
      "source": [
        "# each sequence is 40 in length\n",
        "print(len(sequences[0]), sequences[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40 [50, 77, 24, 48, 17, 13, 54, 117, 48, 13, 24, 95, 120, 72, 77, 37, 117, 34, 24, 13, 4, 48, 87, 87, 45, 7, 112, 120, 2, 77, 117, 4, 112, 120, 13, 4, 120, 77, 24, 120]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHXmmVcKaVtr",
        "colab_type": "text"
      },
      "source": [
        "#### Create X and y\n",
        "* Save the sequence number, character codes in each sequence and their location in each sequence in a boolian matrix\n",
        "* Save the next character code after each sequence as an index pointer in a boolian matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUoyd1nySjtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "# Padding! initialize everything with False\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "test = []\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        # x[sequence#[0:137], char index in sequence[0:40], char encoded value[0:121]]\n",
        "        # in the last dimension of x only 40 out of 121 will be True\n",
        "        # 1 is stored as boolean type\n",
        "        x[i,t,char] = 1\n",
        "        test.append((i,t,char))\n",
        "    # y[sequence, next character after sequence in embeded]   \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMBX5AwXSjto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e692e588-986a-4cac-926a-4045c37c1c66"
      },
      "source": [
        "# each sequence is on one row of axis=0, and will have boolean value(T/F)\n",
        "# (sequence#_index, char_loc_index, char_code_index)\n",
        "x.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 40, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJH4cRcBSjtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9095e221-44ce-44b5-9a24-25fcbc2edeff"
      },
      "source": [
        "# (sequence#_index, char_code_index)\n",
        "y.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVUMWndgTwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ac1796a6-19ab-46af-fbe8-781fb90bb09b"
      },
      "source": [
        "# i is a tuple of indices that is for each character of each sequence in the flattened text\n",
        "# first 5 characters of the first sequence\n",
        "for i in test[:5]:\n",
        "  print(x[i])\n",
        "test[-5:]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(178373, 35, 112),\n",
              " (178373, 36, 112),\n",
              " (178373, 37, 120),\n",
              " (178373, 38, 29),\n",
              " (178373, 39, 112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91wThegSjtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "# x is the input sequence\n",
        "model = Sequential()\n",
        "\n",
        "\"\"\"number of rows does not need to be defined in \n",
        "input_shape as it's similar to batch size and it's dynamic\n",
        "\"\"\"\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "# y is the next character after the sequence\n",
        "model.add(Dense(units=len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnr7pu8q3Y4",
        "colab_type": "text"
      },
      "source": [
        "* n=input dimension=len(chars)=121, maxlen says that only 40 of the 121 inputs are True\n",
        "\n",
        "* m=# of lstm in layer, # paramters = 4*(n+m+1)*m\n",
        "* output parameters = (128 parallel lstm + 1 bias)* 121 (character set or output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ar77LC3qto-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc201c20-f709-499b-e93e-06a6e268dcb6"
      },
      "source": [
        "4*(121+128+1)*128, (128+1)*121"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128000, 15609)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRH_nrjjoKsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "342e8035-9d0d-4152-889e-96ccd3fa9c31"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               128000    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 121)               15609     \n",
            "=================================================================\n",
            "Total params: 143,609\n",
            "Trainable params: 143,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIHVwXmSjt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    \"\"\"\n",
        "    It normalizes the array of preds to proba (with their sum equal to 1)\n",
        "    Then it picks the index of the first maximum based on a random draw with the proba array weight\n",
        "    \"\"\"\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    \n",
        "    # Null operation\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    \n",
        "    # Normalize to the sum of one for the array of probabilities\n",
        "    # exp_preds=preds is an array\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    \n",
        "    #multinomial(# of experiments, list of probability of each outcome, number of output repititions)\n",
        "    # len(probas)=len(preds): showing how many times each possibility was selected\n",
        "    probas = np.random.multinomial(1, preds, 1)   #ex/ probas=[0,0,1,0,0]\n",
        "    \n",
        "    # Returns the indices of the maximum values along an axis.\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMPoUe0-MBCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dcc965ef-378d-441f-dad3-f699575bd225"
      },
      "source": [
        "np.random.multinomial(1, [1/3]*3, 2)\n",
        "ss = np.random.multinomial(1, [1/6, 1/3, 1/2], 1)\n",
        "print(ss)\n",
        "np.argmax(ss)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJL0gA0Sjt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    # Random prompt in order to grab a 40 consecutive character sample as the seed\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        # 400 is the length of generated text\n",
        "        # create a quary sequence:\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        # Predict the next character after the sequence\n",
        "        # preds is an array of length 121 \n",
        "        # with numbers between 0 and 1 \n",
        "        #corresponding to the strength of the prediction of each of 121 characters\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        # update the seed by moving one character forward\n",
        "        sentence = sentence[1:] + next_char\n",
        "        generated += next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print(\"\\n**********\\n\", generated)\n",
        "    print()\n",
        "    print(\"\\n\\nlast character of this epoch preds:\\n\", len(preds), type(preds))\n",
        "    print(preds)\n",
        "\n",
        "# After each epoch generates a brand new 400 characters out of a 40 character seed\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoJKLihxSjt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4bb17ad-34dc-4c97-a8cd-7bdd0e399a7a"
      },
      "source": [
        "# fit the models\n",
        "model.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=2,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "5565/5575 [============================>.] - ETA: 0s - loss: 1.7404\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"a Super Bowl hangover. They’re more conc\"\n",
            "a Super Bowl hangover. They’re more concreatly to shople that the ins “all factmyy, you have and so excepteds to state encearh empirlatical artimoss opan and just that hose which Elambain Dalolins Biless sny Asloa and Ukirigary Pott only presidents: known of desiwn had on the thetterves witn nises is who sag to aly not back for at the iss conflenings surges foriage that extitial Kassingly wabrist, and Trump stagomed afferent after a fac\n",
            "**********\n",
            " a Super Bowl hangover. They’re more concreatly to shople that the ins “all factmyy, you have and so excepteds to state encearh empirlatical artimoss opan and just that hose which Elambain Dalolins Biless sny Asloa and Ukirigary Pott only presidents: known of desiwn had on the thetterves witn nises is who sag to aly not back for at the iss conflenings surges foriage that extitial Kassingly wabrist, and Trump stagomed afferent after a fac\n",
            "\n",
            "\n",
            "\n",
            "last character of this epoch preds:\n",
            " 121 <class 'numpy.ndarray'>\n",
            "[1.58518390e-06 2.78256550e-11 1.20427546e-06 1.61212430e-08\n",
            " 1.56071007e-01 7.58296181e-08 6.51143864e-03 8.52655212e-04\n",
            " 5.86305537e-09 2.48869333e-06 2.31312851e-11 1.54018664e-09\n",
            " 2.81597783e-11 9.26617626e-03 7.63251240e-09 1.00526172e-07\n",
            " 7.05950640e-07 1.31317601e-01 4.71209471e-09 2.05915876e-05\n",
            " 7.67938269e-10 9.67767306e-08 3.68084140e-11 5.13614751e-08\n",
            " 1.06271841e-01 2.45025771e-11 2.28249780e-11 1.86845159e-06\n",
            " 3.44725521e-07 3.66764073e-03 7.23791871e-09 9.29117689e-08\n",
            " 2.22094428e-07 3.18965562e-07 4.93642129e-02 4.49196477e-06\n",
            " 3.23984173e-11 6.33190796e-02 4.50185965e-11 1.36830238e-07\n",
            " 2.30144862e-11 1.04828977e-07 2.80471046e-11 2.55491635e-11\n",
            " 1.10696385e-06 1.04186229e-05 4.81240932e-06 1.64585456e-10\n",
            " 2.28163898e-01 3.85819504e-07 8.08166988e-07 3.64497104e-12\n",
            " 7.15794158e-07 7.25933257e-03 2.97881337e-03 1.20164936e-08\n",
            " 1.92741756e-09 6.60845068e-09 5.75558943e-07 8.77545592e-09\n",
            " 1.02930744e-06 2.91505603e-05 3.03856039e-04 3.28439711e-07\n",
            " 2.99299960e-11 5.83229678e-08 2.02185717e-07 1.05026754e-06\n",
            " 2.77618994e-03 2.60649222e-06 4.30696902e-11 3.20337179e-08\n",
            " 1.94726020e-01 3.46759878e-11 1.30311004e-04 2.64956737e-11\n",
            " 1.40064635e-11 2.36075066e-04 3.11432383e-03 5.20717822e-06\n",
            " 6.07981576e-07 4.54398942e-05 4.49629049e-11 2.90807187e-11\n",
            " 8.32968539e-08 6.89847512e-10 3.57252389e-07 2.76411782e-09\n",
            " 3.15008436e-11 8.59920192e-07 3.07232767e-07 2.37098157e-05\n",
            " 1.53290311e-10 3.97509803e-11 7.42500333e-07 1.71378311e-02\n",
            " 3.82622716e-08 5.82991390e-07 9.17728432e-03 1.26400543e-08\n",
            " 3.02190308e-08 3.15926035e-11 2.88675143e-11 5.39289999e-07\n",
            " 7.90647988e-04 3.62572337e-08 7.10823826e-08 5.80443036e-08\n",
            " 1.03631442e-06 3.29355737e-10 9.67180995e-07 9.36042303e-12\n",
            " 1.05720159e-04 5.16708521e-03 3.70679209e-06 1.51465628e-07\n",
            " 2.48292636e-11 1.10272050e-03 1.28527056e-08 1.87574912e-07\n",
            " 1.56605965e-05]\n",
            "5575/5575 [==============================] - 37s 7ms/step - loss: 1.7407\n",
            "Epoch 2/2\n",
            "5567/5575 [============================>.] - ETA: 0s - loss: 1.7085\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"or promotions at any time. All Print Pro\"\n",
            "or promotions at any time. All Print Pronatiesa if des inter offered to ture anyerary jaugiewhed erecuped honws” couser his cale or harply in O puried the realeving for any deals for the prusicusion, ight affectige.\n",
            "\n",
            "Bitco and out a count, for vimering in the news. Ithisted adroused could that heptting it a fighali draw a trains to winnes a grombaries — is in ispressive. He sble unner will milial interviver her to Amisiding tould by rec\n",
            "**********\n",
            " or promotions at any time. All Print Pronatiesa if des inter offered to ture anyerary jaugiewhed erecuped honws” couser his cale or harply in O puried the realeving for any deals for the prusicusion, ight affectige.\n",
            "\n",
            "Bitco and out a count, for vimering in the news. Ithisted adroused could that heptting it a fighali draw a trains to winnes a grombaries — is in ispressive. He sble unner will milial interviver her to Amisiding tould by rec\n",
            "\n",
            "\n",
            "\n",
            "last character of this epoch preds:\n",
            " 121 <class 'numpy.ndarray'>\n",
            "[3.8159953e-08 6.7221839e-10 1.0725017e-07 1.8708357e-06 1.7936760e-01\n",
            " 9.3846796e-07 2.9048981e-04 9.0886303e-04 6.6725686e-10 7.3723786e-04\n",
            " 4.9921978e-10 4.1643016e-06 5.9322158e-10 6.0821013e-03 9.6226076e-09\n",
            " 2.4605455e-07 2.0497985e-06 3.4917216e-04 6.6177614e-07 2.0020423e-04\n",
            " 1.2910478e-07 3.5895562e-07 4.6648807e-10 4.2896801e-05 1.2159132e-02\n",
            " 6.3253830e-10 6.0940009e-10 4.2464394e-06 8.8128962e-07 1.4846553e-01\n",
            " 1.9716659e-07 5.8545356e-07 1.4695230e-06 4.0665648e-07 2.8505810e-02\n",
            " 1.0543750e-04 8.3400514e-10 8.1729785e-02 2.2624836e-06 1.6079789e-06\n",
            " 4.4594783e-10 1.7776865e-05 3.4569395e-10 5.7084071e-10 3.0446261e-06\n",
            " 2.6741452e-06 7.8273791e-07 3.4757957e-08 1.4830685e-02 4.9049681e-06\n",
            " 5.0004319e-06 6.7244079e-12 7.9483300e-07 8.7222559e-03 4.9684923e-03\n",
            " 7.1482118e-06 1.8102904e-10 3.5492153e-07 1.6013786e-06 4.6656310e-09\n",
            " 1.8927279e-08 3.3631442e-09 7.8132011e-02 8.1473136e-06 5.5770649e-10\n",
            " 5.8514433e-07 2.4909352e-06 6.4118860e-05 5.4482916e-05 2.4911230e-06\n",
            " 7.9366380e-10 1.0448131e-06 2.5014234e-01 6.5464439e-10 7.4364286e-04\n",
            " 6.4521821e-10 7.4221610e-12 1.5033644e-03 1.5889802e-04 1.5758481e-04\n",
            " 3.0435796e-05 3.2299526e-05 5.9587407e-10 4.6690296e-10 2.3843512e-07\n",
            " 2.8575004e-08 1.7839710e-06 2.7923638e-06 6.2380279e-10 2.9955972e-06\n",
            " 2.2540273e-05 3.9439991e-02 4.0475290e-09 5.1424215e-10 2.6333796e-06\n",
            " 4.4062264e-02 2.4034978e-06 1.9927149e-05 1.2895590e-02 2.5693837e-06\n",
            " 8.6976974e-07 3.7597650e-10 8.0685597e-10 2.5698677e-07 5.1068645e-02\n",
            " 2.2187558e-05 5.2702757e-07 8.4098059e-07 5.8509945e-06 3.9233736e-09\n",
            " 8.9743016e-06 1.9223670e-10 1.6186545e-02 6.3724662e-03 5.4458683e-06\n",
            " 7.5734465e-06 7.9466506e-10 6.1508210e-04 4.6736044e-08 3.1139364e-06\n",
            " 1.0679830e-02]\n",
            "5575/5575 [==============================] - 37s 7ms/step - loss: 1.7083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff265f4a470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vh4ElzDSjuB",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use a Keras LSTM to generate text on today's assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rys0_j8CSjuD",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "    * Sequence Problems:\n",
        "        - Time Series (like Stock Prices, Weather, etc.)\n",
        "        - Text Classification\n",
        "        - Text Generation\n",
        "        - And many more! :D\n",
        "    * LSTMs are generally preferred over RNNs for most problems\n",
        "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
        "    * Keras has LSTMs/RNN layer types implemented nicely\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
        "    * Shape of input data is very important\n",
        "    * Can take a while to train\n",
        "    * You can use it to write movie scripts. :P "
      ]
    }
  ]
}